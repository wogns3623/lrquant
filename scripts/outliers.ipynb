{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a01c5c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jhkim/workspace/lrquant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhkim/workspace/lrquant/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a720990f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import typing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.style as mplstyle\n",
    "\n",
    "mplstyle.use(\"fast\")\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "if \"CUDA_VISIBLE_DEVICES\" in os.environ:\n",
    "    print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "\n",
    "torch.set_printoptions(precision=4, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3772ac69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2658e568f940429c9c6822eda432d9fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  32000\n",
      "Loading train from ./cache/wikitext2_1_2/dataloader.cache\n"
     ]
    }
   ],
   "source": [
    "from models.LMClass import LMClass\n",
    "\n",
    "lm = LMClass(\"./huggingface/llama-2-7b-hf\")\n",
    "\n",
    "\n",
    "from datautils import get_loaders\n",
    "\n",
    "dataloader, _ = get_loaders(\n",
    "    name=\"wikitext2\",\n",
    "    nsamples=128,\n",
    "    seed=2,\n",
    "    model=lm.model_name,\n",
    "    seqlen=lm.seqlen,\n",
    "    cache_dir=f\"./cache/wikitext2_1_2/dataloader.cache\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5a18f202",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 torch.Size([2048, 4096]) tensor([[0.8525, 0.6582, 0.4890, 0.4175, 0.3965],\n",
      "        [1.5449, 1.5010, 0.3130, 0.2810, 0.1371],\n",
      "        [1.8750, 1.7295, 0.3628, 0.2742, 0.2263],\n",
      "        ...,\n",
      "        [1.0098, 0.9160, 0.2180, 0.2180, 0.1641],\n",
      "        [1.1367, 1.0166, 0.2620, 0.2103, 0.1593],\n",
      "        [0.6553, 0.2856, 0.2520, 0.2360, 0.1997]], device='cuda:0',\n",
      "       dtype=torch.float16) tensor([0.0059, 0.0025, 0.0022,  ..., 0.0004, 0.0004, 0.0004], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "0 1 torch.Size([2048, 4096]) tensor([[  712.0000,   425.2500,    33.9688,    29.2344,    17.7500],\n",
      "        [    0.4214,     0.3804,     0.3779,     0.3391,     0.2798],\n",
      "        [    0.6948,     0.6738,     0.5356,     0.4800,     0.4697],\n",
      "        ...,\n",
      "        [    0.7759,     0.6943,     0.4829,     0.4543,     0.4136],\n",
      "        [    0.9575,     0.9341,     0.6206,     0.5732,     0.5146],\n",
      "        [    0.5688,     0.5400,     0.4834,     0.4805,     0.4316]],\n",
      "       device='cuda:0', dtype=torch.float16) tensor([   inf,    inf, 0.0033,  ..., 0.0010, 0.0010, 0.0010], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "0 2 torch.Size([2048, 4096]) tensor([[  716.5000,   428.0000,    34.4688,    29.7500,    17.5781],\n",
      "        [    0.7441,     0.5918,     0.4165,     0.4150,     0.4099],\n",
      "        [    0.9033,     0.8945,     0.7207,     0.7090,     0.5562],\n",
      "        ...,\n",
      "        [    1.0654,     0.9307,     0.6885,     0.6152,     0.6118],\n",
      "        [    1.1172,     1.0400,     0.7563,     0.7324,     0.6636],\n",
      "        [    1.0840,     0.7969,     0.6738,     0.6694,     0.5840]],\n",
      "       device='cuda:0', dtype=torch.float16) tensor([   inf,    inf, 0.0062,  ..., 0.0018, 0.0018, 0.0017], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "0 3 torch.Size([2048, 4096]) tensor([[  716.5000,   428.0000,    34.2812,    29.7969,    17.5000],\n",
      "        [    2.2012,     1.0332,     0.6670,     0.6396,     0.6357],\n",
      "        [    1.5215,     1.0225,     0.8140,     0.7832,     0.6675],\n",
      "        ...,\n",
      "        [    1.5000,     1.2061,     0.9019,     0.8008,     0.7778],\n",
      "        [    1.3652,     1.2549,     0.9463,     0.9321,     0.8369],\n",
      "        [    2.0918,     1.0967,     0.9360,     0.9219,     0.8481]],\n",
      "       device='cuda:0', dtype=torch.float16) tensor([   inf,    inf, 0.0116,  ..., 0.0033, 0.0032, 0.0032], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "0 4 torch.Size([2048, 4096]) tensor([[723.5000, 431.7500,  35.7500,  31.7344,  16.3125],\n",
      "        [  2.7910,   1.3867,   0.9834,   0.9243,   0.8970],\n",
      "        [  1.4512,   1.0488,   1.0156,   0.8438,   0.7871],\n",
      "        ...,\n",
      "        [  1.8672,   1.5000,   1.2969,   1.0254,   1.0078],\n",
      "        [  1.5781,   1.4844,   1.2373,   1.1992,   1.1523],\n",
      "        [  1.8896,   1.4414,   1.1592,   1.1338,   1.0264]], device='cuda:0',\n",
      "       dtype=torch.float16) tensor([   inf,    inf, 0.0195,  ..., 0.0060, 0.0060, 0.0058], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "0 5 torch.Size([2048, 4096]) tensor([[723.5000, 431.7500,  35.2812,  31.7031,  16.2500],\n",
      "        [  3.4336,   1.6475,   1.2939,   1.2676,   1.2148],\n",
      "        [  1.6006,   1.1045,   1.0830,   1.0283,   0.9326],\n",
      "        ...,\n",
      "        [  1.9639,   1.8428,   1.6943,   1.3506,   1.1309],\n",
      "        [  1.8223,   1.3984,   1.3789,   1.3242,   1.2070],\n",
      "        [  1.7598,   1.4922,   1.2334,   1.0752,   1.0664]], device='cuda:0',\n",
      "       dtype=torch.float16) tensor([   inf,    inf, 0.0282,  ..., 0.0091, 0.0091, 0.0084], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "0 6 torch.Size([2048, 4096]) tensor([[723.5000, 431.7500,  35.1250,  31.5469,  16.2812],\n",
      "        [  4.5000,   1.9980,   1.7461,   1.6807,   1.5527],\n",
      "        [  1.1797,   1.1260,   1.1201,   1.1172,   1.0615],\n",
      "        ...,\n",
      "        [  2.3320,   2.1504,   1.8936,   1.4238,   1.2246],\n",
      "        [  2.3828,   1.7637,   1.5615,   1.4521,   1.4326],\n",
      "        [  2.2773,   1.6650,   1.3730,   1.2246,   1.2217]], device='cuda:0',\n",
      "       dtype=torch.float16) tensor([   inf,    inf, 0.0540,  ..., 0.0117, 0.0108, 0.0107], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "0 7 torch.Size([2048, 4096]) tensor([[723.5000, 431.7500,  34.3750,  31.5469,  16.2500],\n",
      "        [  5.4023,   2.3242,   2.2305,   1.8809,   1.8750],\n",
      "        [  2.9492,   1.5264,   1.4082,   1.2598,   1.2393],\n",
      "        ...,\n",
      "        [  2.4492,   2.4277,   2.1152,   1.6543,   1.5215],\n",
      "        [  2.7383,   1.9238,   1.7168,   1.5830,   1.5615],\n",
      "        [  2.4492,   1.8486,   1.5381,   1.3115,   1.2627]], device='cuda:0',\n",
      "       dtype=torch.float16) tensor([   inf,    inf, 0.0852,  ..., 0.0162, 0.0147, 0.0146], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "0 8 torch.Size([2048, 4096]) tensor([[723.5000, 431.7500,  34.2188,  31.4219,  16.2969],\n",
      "        [  5.1172,   2.5820,   2.4922,   2.2402,   1.9443],\n",
      "        [  3.5371,   1.9258,   1.6113,   1.4404,   1.3799],\n",
      "        ...,\n",
      "        [  3.5527,   2.8379,   2.5430,   1.8115,   1.7764],\n",
      "        [  3.1973,   2.4062,   1.9434,   1.7617,   1.6582],\n",
      "        [  2.7051,   1.9434,   1.7393,   1.5107,   1.5078]], device='cuda:0',\n",
      "       dtype=torch.float16) tensor([   inf,    inf, 0.1777,  ..., 0.0215, 0.0214, 0.0178], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "0 9 torch.Size([2048, 4096]) tensor([[723.5000, 431.7500,  32.7500,  31.4062,  16.1875],\n",
      "        [  5.7773,   2.9199,   2.7539,   2.4570,   1.9541],\n",
      "        [  4.3828,   2.2617,   1.8789,   1.6982,   1.6621],\n",
      "        ...,\n",
      "        [  3.1992,   3.0039,   2.4590,   1.9814,   1.9199],\n",
      "        [  3.8184,   2.4922,   2.1426,   2.0586,   1.8545],\n",
      "        [  1.9678,   1.8857,   1.8652,   1.8379,   1.6309]], device='cuda:0',\n",
      "       dtype=torch.float16) tensor([   inf,    inf, 0.2289,  ..., 0.0306, 0.0246, 0.0210], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "0 10 torch.Size([2048, 4096]) tensor([[723.5000, 431.7500,  32.1250,  31.3594,  16.2344],\n",
      "        [  6.2148,   3.5977,   3.2070,   2.9121,   2.3027],\n",
      "        [  5.2461,   2.8828,   2.4102,   2.0488,   1.7861],\n",
      "        ...,\n",
      "        [  2.9531,   2.8125,   2.5898,   2.2598,   2.1367],\n",
      "        [  4.0469,   2.5332,   2.3164,   2.1445,   2.1348],\n",
      "        [  2.0605,   1.9629,   1.9502,   1.8545,   1.7686]], device='cuda:0',\n",
      "       dtype=torch.float16) tensor([   inf,    inf, 0.2406,  ..., 0.0360, 0.0355, 0.0245], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "0 11 torch.Size([2048, 4096]) tensor([[723.5000, 431.7500,  31.1094,  30.1250,  16.0312],\n",
      "        [  6.6406,   4.2305,   3.6289,   2.9824,   2.5898],\n",
      "        [  5.8984,   3.3906,   2.8359,   2.7402,   2.0352],\n",
      "        ...,\n",
      "        [  3.3496,   3.0566,   2.7949,   2.3027,   2.1680],\n",
      "        [  4.3438,   2.9551,   2.8203,   2.7090,   2.3301],\n",
      "        [  2.3086,   2.2422,   2.1895,   2.1797,   2.0762]], device='cuda:0',\n",
      "       dtype=torch.float16) tensor([   inf,    inf, 0.2406,  ..., 0.0433, 0.0414, 0.0349], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "0 12 torch.Size([2048, 4096]) tensor([[724.0000, 431.7500,  30.9531,  27.4844,  15.8203],\n",
      "        [  7.5234,   4.7578,   4.1641,   2.8574,   2.7383],\n",
      "        [  7.8984,   3.7539,   3.3652,   3.2578,   2.1934],\n",
      "        ...,\n",
      "        [  4.1406,   3.4180,   2.9062,   2.3652,   2.3164],\n",
      "        [  4.7070,   3.5469,   3.1875,   2.9707,   2.4277],\n",
      "        [  2.5527,   2.5254,   2.5078,   2.1523,   2.1406]], device='cuda:0',\n",
      "       dtype=torch.float16) tensor([   inf,    inf, 0.2429,  ..., 0.0466, 0.0460, 0.0449], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "0 13 torch.Size([2048, 4096]) tensor([[724.5000, 431.7500,  30.7969,  24.7656,  15.6172],\n",
      "        [ 10.1094,   5.4688,   4.9570,   3.3125,   3.3105],\n",
      "        [  9.4922,   4.4062,   4.1250,   3.7754,   2.5234],\n",
      "        ...,\n",
      "        [  4.3594,   3.4082,   2.8477,   2.4141,   2.3613],\n",
      "        [  5.8125,   5.1172,   3.4727,   3.3145,   2.5938],\n",
      "        [  3.9355,   3.0742,   2.9121,   2.5977,   2.0977]], device='cuda:0',\n",
      "       dtype=torch.float16) tensor([   inf,    inf, 0.2703,  ..., 0.0580, 0.0579, 0.0577], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "0 14 torch.Size([2048, 4096]) tensor([[724.5000, 431.7500,  30.6094,  22.9688,  15.4922],\n",
      "        [ 12.2031,   5.8711,   5.2344,   3.6641,   3.5352],\n",
      "        [ 11.2656,   5.0469,   4.9023,   4.2266,   2.6797],\n",
      "        ...,\n",
      "        [  6.2227,   4.0234,   3.2324,   2.7930,   2.5781],\n",
      "        [  6.9648,   5.4570,   3.6602,   3.5410,   2.7012],\n",
      "        [  4.8984,   3.3750,   3.3730,   2.7812,   2.3145]], device='cuda:0',\n",
      "       dtype=torch.float16) tensor([   inf,    inf, 0.2766,  ..., 0.0745, 0.0737, 0.0716], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "0 15 torch.Size([2048, 4096]) tensor([[724.5000, 431.7500,  30.1562,  20.4062,  15.2500],\n",
      "        [ 12.7109,   6.9570,   5.7109,   4.2578,   4.1484],\n",
      "        [ 13.1016,   7.0664,   5.5430,   5.3828,   3.3672],\n",
      "        ...,\n",
      "        [  7.6328,   4.6094,   3.5059,   3.3223,   2.9844],\n",
      "        [  7.6445,   5.9570,   4.0391,   3.5957,   3.0449],\n",
      "        [  5.6094,   4.0312,   3.8828,   2.9902,   2.3672]], device='cuda:0',\n",
      "       dtype=torch.float16) tensor([   inf,    inf, 0.3008,  ..., 0.0876, 0.0835, 0.0811], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "0 16 torch.Size([2048, 4096]) tensor([[728.5000, 431.7500,  28.5469,  20.9531,  13.0469],\n",
      "        [ 15.1328,   7.7734,   6.5508,   5.5156,   5.2695],\n",
      "        [ 15.1328,   8.7578,   6.4961,   6.0391,   3.9160],\n",
      "        ...,\n",
      "        [  7.4375,   5.3984,   3.9023,   3.6934,   3.5078],\n",
      "        [  6.5078,   6.4688,   4.5078,   3.8555,   3.3105],\n",
      "        [  4.7812,   4.1328,   3.7773,   3.1738,   2.8828]], device='cuda:0',\n",
      "       dtype=torch.float16) tensor([   inf,    inf, 0.4038,  ..., 0.1340, 0.1307, 0.1157], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "0 17 torch.Size([2048, 4096]) tensor([[728.5000, 431.7500,  28.3750,  18.2969,  12.6562],\n",
      "        [ 16.8750,   8.8906,   7.6055,   6.9883,   6.7266],\n",
      "        [ 18.7969,  10.5156,   7.1055,   6.7812,   5.1836],\n",
      "        ...,\n",
      "        [  8.4219,   5.8164,   4.2852,   3.9160,   3.5312],\n",
      "        [  8.3047,   6.8945,   4.9258,   4.0703,   3.8672],\n",
      "        [  5.7227,   5.2188,   4.4102,   3.3809,   3.0312]], device='cuda:0',\n",
      "       dtype=torch.float16) tensor([   inf,    inf, 0.4822,  ..., 0.1533, 0.1404, 0.1399], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "0 18 torch.Size([2048, 4096]) tensor([[728.5000, 431.5000,  28.2188,  16.1250,  12.4219],\n",
      "        [ 20.9844,   9.3984,   8.2188,   7.8086,   7.6250],\n",
      "        [ 23.2500,  11.6953,   7.8320,   7.2539,   6.5586],\n",
      "        ...,\n",
      "        [ 11.5781,   6.4609,   4.8906,   4.7188,   4.1953],\n",
      "        [ 10.5156,   7.1680,   5.6445,   4.5312,   4.3047],\n",
      "        [  6.9414,   5.8281,   4.8555,   4.3906,   3.5332]], device='cuda:0',\n",
      "       dtype=torch.float16) tensor([   inf,    inf, 0.7002,  ..., 0.1824, 0.1753, 0.1721], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "0 19 torch.Size([2048, 4096]) tensor([[729.0000, 430.5000,  25.0938,  14.9453,  11.8906],\n",
      "        [ 27.2812,  10.2422,   9.9531,   9.9062,   9.2656],\n",
      "        [ 30.0469,  13.6953,   8.6484,   8.4922,   8.0469],\n",
      "        ...,\n",
      "        [ 13.1172,   7.6250,   5.5078,   5.3555,   4.7578],\n",
      "        [ 12.1875,   8.1641,   6.3047,   5.1641,   5.1562],\n",
      "        [  6.7695,   6.2617,   5.2422,   4.6562,   3.9375]], device='cuda:0',\n",
      "       dtype=torch.float16) tensor([   inf,    inf, 0.8491,  ..., 0.2146, 0.2131, 0.2106], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "0 20 torch.Size([2048, 4096]) tensor([[729.0000, 430.5000,  25.0000,  13.5938,  11.7891],\n",
      "        [ 28.9219,  11.2969,  10.8516,  10.5078,   9.3984],\n",
      "        [ 34.0625,  15.3906,   9.7891,   9.2422,   8.8594],\n",
      "        ...,\n",
      "        [ 13.2109,   8.3359,   6.6016,   5.9219,   5.0352],\n",
      "        [ 10.2812,   8.8203,   6.8047,   5.7500,   5.4023],\n",
      "        [  6.4844,   5.3828,   5.2422,   5.1484,   4.3242]], device='cuda:0',\n",
      "       dtype=torch.float16) tensor([   inf,    inf, 1.2188,  ..., 0.2681, 0.2598, 0.2563], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "0 21 torch.Size([2048, 4096]) tensor([[729.5000, 430.5000,  24.8750,  11.7031,  11.3203],\n",
      "        [ 31.8750,  14.8438,  12.6328,  12.5625,  10.0156],\n",
      "        [ 38.6562,  17.7969,  11.2188,  10.0703,   9.6719],\n",
      "        ...,\n",
      "        [ 13.8438,   8.9688,   6.8750,   5.7930,   5.1133],\n",
      "        [ 10.5781,   9.2812,   7.0273,   6.2930,   5.6133],\n",
      "        [  6.8594,   5.6914,   5.5312,   5.3242,   4.7148]], device='cuda:0',\n",
      "       dtype=torch.float16) tensor([   inf,    inf, 1.5625,  ..., 0.3196, 0.3149, 0.3140], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "0 22 torch.Size([2048, 4096]) tensor([[730.0000, 430.7500,  24.7500,  11.4844,  10.2500],\n",
      "        [ 39.0938,  15.7969,  13.7266,  13.1641,  10.8828],\n",
      "        [ 45.4688,  18.8906,  12.4531,  10.6719,  10.3203],\n",
      "        ...,\n",
      "        [ 16.6562,   9.3125,   7.6641,   6.3359,   5.3281],\n",
      "        [ 11.2578,   9.6328,   7.0273,   6.7305,   6.3164],\n",
      "        [  6.8320,   5.8477,   5.6055,   4.8828,   4.8008]], device='cuda:0',\n",
      "       dtype=torch.float16) tensor([   inf,    inf, 2.0352,  ..., 0.3896, 0.3845, 0.3823], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "0 23 torch.Size([2048, 4096]) tensor([[730.5000, 430.7500,  24.8125,  11.2891,  10.2188],\n",
      "        [ 43.5000,  16.3281,  14.2812,  13.7969,  11.5000],\n",
      "        [ 49.2500,  19.1719,  13.2422,  10.8359,  10.6484],\n",
      "        ...,\n",
      "        [ 17.4062,   9.6406,   7.7461,   6.7031,   5.8516],\n",
      "        [ 11.3125,   9.8281,   7.2891,   6.9414,   6.8633],\n",
      "        [  7.0938,   6.1484,   6.0820,   5.1680,   5.0352]], device='cuda:0',\n",
      "       dtype=torch.float16) tensor([   inf,    inf, 2.3613,  ..., 0.4326, 0.4185, 0.4104], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "0 24 torch.Size([2048, 4096]) tensor([[730.5000, 430.7500,  24.7188,  11.0625,  10.2656],\n",
      "        [ 50.4688,  17.6875,  15.8281,  14.2891,  11.9219],\n",
      "        [ 61.2500,  20.7500,  15.5312,  11.4922,  11.0234],\n",
      "        ...,\n",
      "        [ 22.2656,  10.3438,   8.4219,   7.2383,   6.9492],\n",
      "        [ 15.0781,  10.1172,   7.7305,   7.3398,   6.8203],\n",
      "        [  7.9453,   7.5117,   6.4414,   6.0859,   5.3672]], device='cuda:0',\n",
      "       dtype=torch.float16) tensor([   inf,    inf, 2.9199,  ..., 0.5317, 0.5308, 0.5249], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "0 25 torch.Size([2048, 4096]) tensor([[730.0000, 430.5000,  24.5938,  10.8828,  10.3359],\n",
      "        [ 56.0312,  18.2500,  16.4531,  14.4141,  12.5312],\n",
      "        [ 63.0625,  20.9531,  15.3203,  11.4453,  11.1406],\n",
      "        ...,\n",
      "        [ 23.7969,  10.6875,   8.9609,   7.9844,   7.2461],\n",
      "        [ 17.3750,  10.6484,   7.8477,   7.6602,   7.1523],\n",
      "        [  8.8672,   7.5938,   6.6602,   6.2969,   5.5781]], device='cuda:0',\n",
      "       dtype=torch.float16) tensor([   inf,    inf, 3.2207,  ..., 0.5938, 0.5889, 0.5830], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "0 26 torch.Size([2048, 4096]) tensor([[730.0000, 430.5000,  24.7812,  10.6250,  10.5078],\n",
      "        [ 62.9375,  19.1562,  17.2656,  14.5078,  12.7188],\n",
      "        [ 67.1250,  21.3750,  15.7656,  11.4297,  10.7188],\n",
      "        ...,\n",
      "        [ 30.9688,  11.2266,   9.6172,   8.5234,   6.5000],\n",
      "        [ 26.0312,  11.6953,   7.9180,   7.2188,   7.0898],\n",
      "        [ 15.7734,   6.9961,   6.2266,   6.1562,   6.0000]], device='cuda:0',\n",
      "       dtype=torch.float16) tensor([   inf,    inf, 3.9023,  ..., 0.6782, 0.6724, 0.6680], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "0 27 torch.Size([2048, 4096]) tensor([[729.5000, 430.0000,  24.6562,  10.6953,  10.1250],\n",
      "        [ 66.8750,  19.4688,  16.9688,  14.5703,  12.3672],\n",
      "        [ 71.4375,  22.0312,  15.6797,  11.6875,  11.1094],\n",
      "        ...,\n",
      "        [ 34.9062,  11.2734,  10.4766,   8.1250,   6.6016],\n",
      "        [ 29.5312,  11.5625,   8.7109,   6.9570,   6.7500],\n",
      "        [ 19.7969,   6.7773,   6.5742,   6.3438,   5.9531]], device='cuda:0',\n",
      "       dtype=torch.float16) tensor([   inf,    inf, 4.2734,  ..., 0.7646, 0.7480, 0.7456], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "0 28 torch.Size([2048, 4096]) tensor([[728.5000, 429.7500,  24.4219,  12.1641,  10.8750],\n",
      "        [ 72.7500,  19.6250,  16.5312,  14.8047,  13.0156],\n",
      "        [ 77.3750,  22.1875,  15.1719,  12.3750,  11.7969],\n",
      "        ...,\n",
      "        [ 43.0625,  11.8828,  11.1875,   8.7109,   6.6719],\n",
      "        [ 35.5938,  11.4297,   9.8906,   7.3711,   7.0977],\n",
      "        [ 28.1250,   8.2344,   6.7773,   6.6016,   6.3438]], device='cuda:0',\n",
      "       dtype=torch.float16) tensor([   inf,    inf, 4.9648,  ..., 0.9146, 0.9038, 0.8359], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "0 29 torch.Size([2048, 4096]) tensor([[723.5000, 427.0000,  23.7656,  14.8594,  11.2656],\n",
      "        [ 76.0625,  18.3125,  14.9453,  14.3750,  13.0859],\n",
      "        [ 80.9375,  21.5312,  13.5938,  13.3438,  11.6641],\n",
      "        ...,\n",
      "        [ 49.8438,  13.1172,  10.1797,   7.6250,   7.6211],\n",
      "        [ 46.1250,  11.7969,  10.5859,   7.5000,   7.4531],\n",
      "        [ 36.5000,  10.1328,   6.6484,   6.4609,   6.3984]], device='cuda:0',\n",
      "       dtype=torch.float16) tensor([   inf,    inf, 5.9414,  ..., 1.0625, 1.0352, 0.9604], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "0 30 torch.Size([2048, 4096]) tensor([[243.2500, 133.5000,  44.4375,   8.2578,   4.2500],\n",
      "        [ 84.0000,  14.4531,  13.4531,  12.8047,  12.4531],\n",
      "        [ 86.3125,  17.6875,  11.9453,   9.9609,   8.7188],\n",
      "        ...,\n",
      "        [ 48.0625,  13.7188,  11.2188,   9.3047,   8.8516],\n",
      "        [ 44.7500,  10.9844,   8.3125,   7.6562,   7.5742],\n",
      "        [ 34.7812,  11.0156,  10.1406,   8.4844,   7.8125]], device='cuda:0',\n",
      "       dtype=torch.float16) tensor([19.3750,  6.9453,  6.7383,  ...,  1.3281,  1.2656,  1.2617],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "0 31 torch.Size([2048, 4096]) tensor([[11.6406, 10.2500,  9.2188,  8.4375,  6.9648],\n",
      "        [36.0938, 26.8438, 26.4844, 22.5781, 21.4062],\n",
      "        [79.8125, 61.2500, 54.5000, 46.2812, 38.5938],\n",
      "        ...,\n",
      "        [23.1094, 21.7656, 17.9062, 15.6562, 13.4219],\n",
      "        [14.6641, 13.8125, 11.5938, 11.3828, 11.0781],\n",
      "        [20.6875, 15.8516, 14.9844, 14.7969, 13.6094]], device='cuda:0',\n",
      "       dtype=torch.float16) tensor([    inf, 30.6250, 18.7812,  ...,  0.9834,  0.4929,  0.3606],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:10, 10.48s/it]\n"
     ]
    }
   ],
   "source": [
    "dataloader_part = dataloader[:1]\n",
    "\n",
    "layer_all_inps = torch.zeros(\n",
    "    (len(dataloader_part), 32, 2048, 4096), dtype=torch.float32\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in tqdm(enumerate(dataloader_part)):\n",
    "        for layer_index, layer in enumerate(lm.model.model.layers):\n",
    "\n",
    "            def layer_hook(batch_index, layer_index):\n",
    "                def _fn(module, input: tuple[torch.Tensor], output):\n",
    "                    out_token_magnitudes = output[0][0].square().mean(dim=1)\n",
    "                    print(\n",
    "                        batch_index,\n",
    "                        layer_index,\n",
    "                        output[0][0].shape,\n",
    "                        output[0][0].abs().topk(5).values,\n",
    "                        out_token_magnitudes.sort(descending=True).values,\n",
    "                    )\n",
    "                    layer_all_inps[batch_index, layer_index] = (\n",
    "                        output[0][0].detach().clone()\n",
    "                    )\n",
    "\n",
    "                return _fn\n",
    "\n",
    "            layer._forward_hooks.clear()\n",
    "            layer.register_forward_hook(layer_hook(batch_idx, layer_index))\n",
    "\n",
    "        lm.model.eval()\n",
    "        lm.model.to(lm.device)\n",
    "        lm.model(batch[0].to(lm.device))\n",
    "        lm.model.cpu()\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0e8dd060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  712.0000,   425.2500,    33.9688,    29.2344,    17.7500],\n",
       "        [    0.4214,     0.3804,     0.3779,     0.3391,     0.2798],\n",
       "        [    0.6948,     0.6738,     0.5356,     0.4800,     0.4697],\n",
       "        ...,\n",
       "        [    0.7759,     0.6943,     0.4829,     0.4543,     0.4136],\n",
       "        [    0.9575,     0.9341,     0.6206,     0.5732,     0.5146],\n",
       "        [    0.5688,     0.5400,     0.4834,     0.4805,     0.4316]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_inps = layer_all_inps[0]\n",
    "layer_all_inps[0][1].abs().topk(5).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba683aa4",
   "metadata": {},
   "source": [
    "## 두 가지 종류의 outlier\n",
    "### outlier token \n",
    "- 전체 input sequence에서 유독 높은 vector magnitude들을 가지는 sequence가 있음\n",
    "- 대부분의 입력 데이터에서 0번 sequence의 token vector magnitude가 큰 값을 가짐\n",
    "    - <bos> 토큰?\n",
    "- 적어도 llama-2-7b 모델에서는 하나 이상의 0번이 아닌 outlier token이 존재함\n",
    "    - 몇 가지 입력 데이터를 확인해본 결과 \".\"이거나 \"\\n\" 토큰이었음\n",
    "- 이 outlier token은 여러 outlier component들을 가지지만, 대부분의 components들은 여전히 0에 가까운 값을 가짐\n",
    "    - 몇 개의 극단적인 outlier component들이 존재\n",
    "        - dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f0be796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2048])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(tensor(0.0059), tensor(23))</td>\n",
       "      <td>(tensor(0.0025), tensor(2))</td>\n",
       "      <td>(tensor(0.0022), tensor(10))</td>\n",
       "      <td>(tensor(0.0021), tensor(71))</td>\n",
       "      <td>(tensor(0.0021), tensor(0))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(tensor(1992.3059), tensor(23))</td>\n",
       "      <td>(tensor(169.0264), tensor(0))</td>\n",
       "      <td>(tensor(0.0033), tensor(526))</td>\n",
       "      <td>(tensor(0.0032), tensor(21))</td>\n",
       "      <td>(tensor(0.0032), tensor(1491))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(tensor(1998.9679), tensor(23))</td>\n",
       "      <td>(tensor(171.1648), tensor(0))</td>\n",
       "      <td>(tensor(0.0062), tensor(1775))</td>\n",
       "      <td>(tensor(0.0060), tensor(1456))</td>\n",
       "      <td>(tensor(0.0060), tensor(472))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(tensor(1998.8000), tensor(23))</td>\n",
       "      <td>(tensor(171.1653), tensor(0))</td>\n",
       "      <td>(tensor(0.0116), tensor(34))</td>\n",
       "      <td>(tensor(0.0115), tensor(1979))</td>\n",
       "      <td>(tensor(0.0107), tensor(904))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(tensor(2014.1641), tensor(23))</td>\n",
       "      <td>(tensor(174.4608), tensor(0))</td>\n",
       "      <td>(tensor(0.0195), tensor(904))</td>\n",
       "      <td>(tensor(0.0187), tensor(1777))</td>\n",
       "      <td>(tensor(0.0186), tensor(1769))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(tensor(2014.1191), tensor(23))</td>\n",
       "      <td>(tensor(174.4566), tensor(0))</td>\n",
       "      <td>(tensor(0.0282), tensor(904))</td>\n",
       "      <td>(tensor(0.0274), tensor(1634))</td>\n",
       "      <td>(tensor(0.0271), tensor(1))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(tensor(2014.0920), tensor(23))</td>\n",
       "      <td>(tensor(174.4552), tensor(0))</td>\n",
       "      <td>(tensor(0.0540), tensor(154))</td>\n",
       "      <td>(tensor(0.0532), tensor(980))</td>\n",
       "      <td>(tensor(0.0443), tensor(531))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(tensor(2014.1165), tensor(23))</td>\n",
       "      <td>(tensor(174.4440), tensor(0))</td>\n",
       "      <td>(tensor(0.0852), tensor(154))</td>\n",
       "      <td>(tensor(0.0650), tensor(1203))</td>\n",
       "      <td>(tensor(0.0643), tensor(1730))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(tensor(2013.9060), tensor(23))</td>\n",
       "      <td>(tensor(174.4408), tensor(0))</td>\n",
       "      <td>(tensor(0.1777), tensor(154))</td>\n",
       "      <td>(tensor(0.1619), tensor(495))</td>\n",
       "      <td>(tensor(0.1182), tensor(980))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(tensor(2013.8350), tensor(23))</td>\n",
       "      <td>(tensor(174.4154), tensor(0))</td>\n",
       "      <td>(tensor(0.2289), tensor(154))</td>\n",
       "      <td>(tensor(0.1808), tensor(495))</td>\n",
       "      <td>(tensor(0.1388), tensor(980))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(tensor(2013.6870), tensor(23))</td>\n",
       "      <td>(tensor(174.4019), tensor(0))</td>\n",
       "      <td>(tensor(0.2407), tensor(154))</td>\n",
       "      <td>(tensor(0.1809), tensor(495))</td>\n",
       "      <td>(tensor(0.1392), tensor(980))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(tensor(2013.6053), tensor(23))</td>\n",
       "      <td>(tensor(174.3587), tensor(0))</td>\n",
       "      <td>(tensor(0.2406), tensor(154))</td>\n",
       "      <td>(tensor(0.1813), tensor(495))</td>\n",
       "      <td>(tensor(0.1391), tensor(980))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(tensor(2013.5688), tensor(23))</td>\n",
       "      <td>(tensor(174.4910), tensor(0))</td>\n",
       "      <td>(tensor(0.2428), tensor(154))</td>\n",
       "      <td>(tensor(0.1854), tensor(495))</td>\n",
       "      <td>(tensor(0.1489), tensor(900))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(tensor(2013.3762), tensor(23))</td>\n",
       "      <td>(tensor(174.6245), tensor(0))</td>\n",
       "      <td>(tensor(0.2703), tensor(154))</td>\n",
       "      <td>(tensor(0.1825), tensor(900))</td>\n",
       "      <td>(tensor(0.1794), tensor(1131))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(tensor(2013.2854), tensor(23))</td>\n",
       "      <td>(tensor(174.5938), tensor(0))</td>\n",
       "      <td>(tensor(0.2766), tensor(154))</td>\n",
       "      <td>(tensor(0.2058), tensor(1))</td>\n",
       "      <td>(tensor(0.2052), tensor(572))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(tensor(2013.0898), tensor(23))</td>\n",
       "      <td>(tensor(174.5507), tensor(0))</td>\n",
       "      <td>(tensor(0.3009), tensor(154))</td>\n",
       "      <td>(tensor(0.3001), tensor(1247))</td>\n",
       "      <td>(tensor(0.2892), tensor(1944))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(tensor(2016.9346), tensor(23))</td>\n",
       "      <td>(tensor(175.7629), tensor(0))</td>\n",
       "      <td>(tensor(0.4039), tensor(619))</td>\n",
       "      <td>(tensor(0.3910), tensor(756))</td>\n",
       "      <td>(tensor(0.3859), tensor(866))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(tensor(2016.9409), tensor(23))</td>\n",
       "      <td>(tensor(175.7262), tensor(0))</td>\n",
       "      <td>(tensor(0.4822), tensor(866))</td>\n",
       "      <td>(tensor(0.4757), tensor(619))</td>\n",
       "      <td>(tensor(0.4675), tensor(1317))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(tensor(2016.7666), tensor(23))</td>\n",
       "      <td>(tensor(175.6513), tensor(0))</td>\n",
       "      <td>(tensor(0.7002), tensor(154))</td>\n",
       "      <td>(tensor(0.6139), tensor(1028))</td>\n",
       "      <td>(tensor(0.6092), tensor(869))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(tensor(2015.2449), tensor(23))</td>\n",
       "      <td>(tensor(175.4837), tensor(0))</td>\n",
       "      <td>(tensor(0.8491), tensor(1134))</td>\n",
       "      <td>(tensor(0.8292), tensor(753))</td>\n",
       "      <td>(tensor(0.8074), tensor(1039))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(tensor(2015.0593), tensor(23))</td>\n",
       "      <td>(tensor(175.4696), tensor(0))</td>\n",
       "      <td>(tensor(1.2191), tensor(1134))</td>\n",
       "      <td>(tensor(1.1622), tensor(32))</td>\n",
       "      <td>(tensor(1.1537), tensor(225))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(tensor(2014.8284), tensor(23))</td>\n",
       "      <td>(tensor(175.6257), tensor(0))</td>\n",
       "      <td>(tensor(1.5628), tensor(686))</td>\n",
       "      <td>(tensor(1.5450), tensor(1013))</td>\n",
       "      <td>(tensor(1.4908), tensor(1134))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(tensor(2014.8403), tensor(23))</td>\n",
       "      <td>(tensor(175.8232), tensor(0))</td>\n",
       "      <td>(tensor(2.0353), tensor(1881))</td>\n",
       "      <td>(tensor(2.0038), tensor(686))</td>\n",
       "      <td>(tensor(1.9623), tensor(1723))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(tensor(2014.6740), tensor(23))</td>\n",
       "      <td>(tensor(175.9987), tensor(0))</td>\n",
       "      <td>(tensor(2.3619), tensor(1881))</td>\n",
       "      <td>(tensor(2.3290), tensor(686))</td>\n",
       "      <td>(tensor(2.3127), tensor(1723))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(tensor(2014.3962), tensor(23))</td>\n",
       "      <td>(tensor(175.9972), tensor(0))</td>\n",
       "      <td>(tensor(2.9193), tensor(1723))</td>\n",
       "      <td>(tensor(2.8940), tensor(686))</td>\n",
       "      <td>(tensor(2.8637), tensor(1881))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>(tensor(2014.3105), tensor(23))</td>\n",
       "      <td>(tensor(175.7673), tensor(0))</td>\n",
       "      <td>(tensor(3.2205), tensor(1881))</td>\n",
       "      <td>(tensor(3.1938), tensor(1723))</td>\n",
       "      <td>(tensor(3.1864), tensor(686))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>(tensor(2014.1714), tensor(23))</td>\n",
       "      <td>(tensor(175.7759), tensor(0))</td>\n",
       "      <td>(tensor(3.9020), tensor(1723))</td>\n",
       "      <td>(tensor(3.8123), tensor(649))</td>\n",
       "      <td>(tensor(3.7915), tensor(686))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>(tensor(2013.9679), tensor(23))</td>\n",
       "      <td>(tensor(175.4980), tensor(0))</td>\n",
       "      <td>(tensor(4.2742), tensor(1723))</td>\n",
       "      <td>(tensor(4.2125), tensor(649))</td>\n",
       "      <td>(tensor(4.1309), tensor(686))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>(tensor(2013.9122), tensor(23))</td>\n",
       "      <td>(tensor(175.1280), tensor(0))</td>\n",
       "      <td>(tensor(4.9628), tensor(649))</td>\n",
       "      <td>(tensor(4.8861), tensor(1723))</td>\n",
       "      <td>(tensor(4.8066), tensor(646))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>(tensor(1951.5186), tensor(23))</td>\n",
       "      <td>(tensor(172.7883), tensor(0))</td>\n",
       "      <td>(tensor(5.9393), tensor(1723))</td>\n",
       "      <td>(tensor(5.8101), tensor(646))</td>\n",
       "      <td>(tensor(5.6162), tensor(649))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>(tensor(19.3713), tensor(0))</td>\n",
       "      <td>(tensor(6.9435), tensor(23))</td>\n",
       "      <td>(tensor(6.7385), tensor(646))</td>\n",
       "      <td>(tensor(6.6244), tensor(753))</td>\n",
       "      <td>(tensor(6.5965), tensor(685))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>(tensor(30.6304), tensor(1776))</td>\n",
       "      <td>(tensor(21.4632), tensor(643))</td>\n",
       "      <td>(tensor(18.7834), tensor(533))</td>\n",
       "      <td>(tensor(18.6839), tensor(1829))</td>\n",
       "      <td>(tensor(18.0727), tensor(647))</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  0                               1  \\\n",
       "0      (tensor(0.0059), tensor(23))     (tensor(0.0025), tensor(2))   \n",
       "1   (tensor(1992.3059), tensor(23))   (tensor(169.0264), tensor(0))   \n",
       "2   (tensor(1998.9679), tensor(23))   (tensor(171.1648), tensor(0))   \n",
       "3   (tensor(1998.8000), tensor(23))   (tensor(171.1653), tensor(0))   \n",
       "4   (tensor(2014.1641), tensor(23))   (tensor(174.4608), tensor(0))   \n",
       "5   (tensor(2014.1191), tensor(23))   (tensor(174.4566), tensor(0))   \n",
       "6   (tensor(2014.0920), tensor(23))   (tensor(174.4552), tensor(0))   \n",
       "7   (tensor(2014.1165), tensor(23))   (tensor(174.4440), tensor(0))   \n",
       "8   (tensor(2013.9060), tensor(23))   (tensor(174.4408), tensor(0))   \n",
       "9   (tensor(2013.8350), tensor(23))   (tensor(174.4154), tensor(0))   \n",
       "10  (tensor(2013.6870), tensor(23))   (tensor(174.4019), tensor(0))   \n",
       "11  (tensor(2013.6053), tensor(23))   (tensor(174.3587), tensor(0))   \n",
       "12  (tensor(2013.5688), tensor(23))   (tensor(174.4910), tensor(0))   \n",
       "13  (tensor(2013.3762), tensor(23))   (tensor(174.6245), tensor(0))   \n",
       "14  (tensor(2013.2854), tensor(23))   (tensor(174.5938), tensor(0))   \n",
       "15  (tensor(2013.0898), tensor(23))   (tensor(174.5507), tensor(0))   \n",
       "16  (tensor(2016.9346), tensor(23))   (tensor(175.7629), tensor(0))   \n",
       "17  (tensor(2016.9409), tensor(23))   (tensor(175.7262), tensor(0))   \n",
       "18  (tensor(2016.7666), tensor(23))   (tensor(175.6513), tensor(0))   \n",
       "19  (tensor(2015.2449), tensor(23))   (tensor(175.4837), tensor(0))   \n",
       "20  (tensor(2015.0593), tensor(23))   (tensor(175.4696), tensor(0))   \n",
       "21  (tensor(2014.8284), tensor(23))   (tensor(175.6257), tensor(0))   \n",
       "22  (tensor(2014.8403), tensor(23))   (tensor(175.8232), tensor(0))   \n",
       "23  (tensor(2014.6740), tensor(23))   (tensor(175.9987), tensor(0))   \n",
       "24  (tensor(2014.3962), tensor(23))   (tensor(175.9972), tensor(0))   \n",
       "25  (tensor(2014.3105), tensor(23))   (tensor(175.7673), tensor(0))   \n",
       "26  (tensor(2014.1714), tensor(23))   (tensor(175.7759), tensor(0))   \n",
       "27  (tensor(2013.9679), tensor(23))   (tensor(175.4980), tensor(0))   \n",
       "28  (tensor(2013.9122), tensor(23))   (tensor(175.1280), tensor(0))   \n",
       "29  (tensor(1951.5186), tensor(23))   (tensor(172.7883), tensor(0))   \n",
       "30     (tensor(19.3713), tensor(0))    (tensor(6.9435), tensor(23))   \n",
       "31  (tensor(30.6304), tensor(1776))  (tensor(21.4632), tensor(643))   \n",
       "\n",
       "                                 2                                3  \\\n",
       "0     (tensor(0.0022), tensor(10))     (tensor(0.0021), tensor(71))   \n",
       "1    (tensor(0.0033), tensor(526))     (tensor(0.0032), tensor(21))   \n",
       "2   (tensor(0.0062), tensor(1775))   (tensor(0.0060), tensor(1456))   \n",
       "3     (tensor(0.0116), tensor(34))   (tensor(0.0115), tensor(1979))   \n",
       "4    (tensor(0.0195), tensor(904))   (tensor(0.0187), tensor(1777))   \n",
       "5    (tensor(0.0282), tensor(904))   (tensor(0.0274), tensor(1634))   \n",
       "6    (tensor(0.0540), tensor(154))    (tensor(0.0532), tensor(980))   \n",
       "7    (tensor(0.0852), tensor(154))   (tensor(0.0650), tensor(1203))   \n",
       "8    (tensor(0.1777), tensor(154))    (tensor(0.1619), tensor(495))   \n",
       "9    (tensor(0.2289), tensor(154))    (tensor(0.1808), tensor(495))   \n",
       "10   (tensor(0.2407), tensor(154))    (tensor(0.1809), tensor(495))   \n",
       "11   (tensor(0.2406), tensor(154))    (tensor(0.1813), tensor(495))   \n",
       "12   (tensor(0.2428), tensor(154))    (tensor(0.1854), tensor(495))   \n",
       "13   (tensor(0.2703), tensor(154))    (tensor(0.1825), tensor(900))   \n",
       "14   (tensor(0.2766), tensor(154))      (tensor(0.2058), tensor(1))   \n",
       "15   (tensor(0.3009), tensor(154))   (tensor(0.3001), tensor(1247))   \n",
       "16   (tensor(0.4039), tensor(619))    (tensor(0.3910), tensor(756))   \n",
       "17   (tensor(0.4822), tensor(866))    (tensor(0.4757), tensor(619))   \n",
       "18   (tensor(0.7002), tensor(154))   (tensor(0.6139), tensor(1028))   \n",
       "19  (tensor(0.8491), tensor(1134))    (tensor(0.8292), tensor(753))   \n",
       "20  (tensor(1.2191), tensor(1134))     (tensor(1.1622), tensor(32))   \n",
       "21   (tensor(1.5628), tensor(686))   (tensor(1.5450), tensor(1013))   \n",
       "22  (tensor(2.0353), tensor(1881))    (tensor(2.0038), tensor(686))   \n",
       "23  (tensor(2.3619), tensor(1881))    (tensor(2.3290), tensor(686))   \n",
       "24  (tensor(2.9193), tensor(1723))    (tensor(2.8940), tensor(686))   \n",
       "25  (tensor(3.2205), tensor(1881))   (tensor(3.1938), tensor(1723))   \n",
       "26  (tensor(3.9020), tensor(1723))    (tensor(3.8123), tensor(649))   \n",
       "27  (tensor(4.2742), tensor(1723))    (tensor(4.2125), tensor(649))   \n",
       "28   (tensor(4.9628), tensor(649))   (tensor(4.8861), tensor(1723))   \n",
       "29  (tensor(5.9393), tensor(1723))    (tensor(5.8101), tensor(646))   \n",
       "30   (tensor(6.7385), tensor(646))    (tensor(6.6244), tensor(753))   \n",
       "31  (tensor(18.7834), tensor(533))  (tensor(18.6839), tensor(1829))   \n",
       "\n",
       "                                 4  \n",
       "0      (tensor(0.0021), tensor(0))  \n",
       "1   (tensor(0.0032), tensor(1491))  \n",
       "2    (tensor(0.0060), tensor(472))  \n",
       "3    (tensor(0.0107), tensor(904))  \n",
       "4   (tensor(0.0186), tensor(1769))  \n",
       "5      (tensor(0.0271), tensor(1))  \n",
       "6    (tensor(0.0443), tensor(531))  \n",
       "7   (tensor(0.0643), tensor(1730))  \n",
       "8    (tensor(0.1182), tensor(980))  \n",
       "9    (tensor(0.1388), tensor(980))  \n",
       "10   (tensor(0.1392), tensor(980))  \n",
       "11   (tensor(0.1391), tensor(980))  \n",
       "12   (tensor(0.1489), tensor(900))  \n",
       "13  (tensor(0.1794), tensor(1131))  \n",
       "14   (tensor(0.2052), tensor(572))  \n",
       "15  (tensor(0.2892), tensor(1944))  \n",
       "16   (tensor(0.3859), tensor(866))  \n",
       "17  (tensor(0.4675), tensor(1317))  \n",
       "18   (tensor(0.6092), tensor(869))  \n",
       "19  (tensor(0.8074), tensor(1039))  \n",
       "20   (tensor(1.1537), tensor(225))  \n",
       "21  (tensor(1.4908), tensor(1134))  \n",
       "22  (tensor(1.9623), tensor(1723))  \n",
       "23  (tensor(2.3127), tensor(1723))  \n",
       "24  (tensor(2.8637), tensor(1881))  \n",
       "25   (tensor(3.1864), tensor(686))  \n",
       "26   (tensor(3.7915), tensor(686))  \n",
       "27   (tensor(4.1309), tensor(686))  \n",
       "28   (tensor(4.8066), tensor(646))  \n",
       "29   (tensor(5.6162), tensor(649))  \n",
       "30   (tensor(6.5965), tensor(685))  \n",
       "31  (tensor(18.0727), tensor(647))  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "magnitudes = layer_inps.square().mean(dim=2)  # [32, 2048]\n",
    "print(magnitudes.shape)\n",
    "topk = magnitudes.topk(5, dim=1)\n",
    "display(pd.DataFrame([list(zip(topk.values[i], topk.indices[i])) for i in range(32)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc25049b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2533</th>\n",
       "      <th>1415</th>\n",
       "      <th>1512</th>\n",
       "      <th>1076</th>\n",
       "      <th>2298</th>\n",
       "      <th>339</th>\n",
       "      <th>3431</th>\n",
       "      <th>2789</th>\n",
       "      <th>2158</th>\n",
       "      <th>3135</th>\n",
       "      <th>...</th>\n",
       "      <th>373</th>\n",
       "      <th>2400</th>\n",
       "      <th>3912</th>\n",
       "      <th>2057</th>\n",
       "      <th>2245</th>\n",
       "      <th>1151</th>\n",
       "      <th>3549</th>\n",
       "      <th>1105</th>\n",
       "      <th>591</th>\n",
       "      <th>1241</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>716.5</td>\n",
       "      <td>428.0</td>\n",
       "      <td>34.46875</td>\n",
       "      <td>29.75</td>\n",
       "      <td>17.578125</td>\n",
       "      <td>14.9375</td>\n",
       "      <td>14.648438</td>\n",
       "      <td>14.304688</td>\n",
       "      <td>12.59375</td>\n",
       "      <td>11.0625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 4096 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    2533   1415      1512   1076       2298     339        3431       2789  \\\n",
       "0  716.5  428.0  34.46875  29.75  17.578125  14.9375  14.648438  14.304688   \n",
       "\n",
       "       2158     3135  ...      373       2400      3912      2057      2245  \\\n",
       "0  12.59375  11.0625  ...  0.000488  0.000458  0.000401  0.000366  0.000343   \n",
       "\n",
       "       1151      3549      1105      591       1241  \n",
       "0  0.000305  0.000305  0.000183  0.000156  0.000076  \n",
       "\n",
       "[1 rows x 4096 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1793</th>\n",
       "      <th>2298</th>\n",
       "      <th>257</th>\n",
       "      <th>1512</th>\n",
       "      <th>339</th>\n",
       "      <th>2789</th>\n",
       "      <th>3135</th>\n",
       "      <th>2522</th>\n",
       "      <th>490</th>\n",
       "      <th>1076</th>\n",
       "      <th>...</th>\n",
       "      <th>1233</th>\n",
       "      <th>381</th>\n",
       "      <th>726</th>\n",
       "      <th>3186</th>\n",
       "      <th>396</th>\n",
       "      <th>1226</th>\n",
       "      <th>285</th>\n",
       "      <th>1989</th>\n",
       "      <th>2419</th>\n",
       "      <th>427</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.733398</td>\n",
       "      <td>0.698242</td>\n",
       "      <td>0.696289</td>\n",
       "      <td>0.665039</td>\n",
       "      <td>0.570312</td>\n",
       "      <td>0.537598</td>\n",
       "      <td>0.537598</td>\n",
       "      <td>0.483643</td>\n",
       "      <td>0.482178</td>\n",
       "      <td>0.429688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 4096 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       1793      2298      257       1512      339       2789      3135  \\\n",
       "0  0.733398  0.698242  0.696289  0.665039  0.570312  0.537598  0.537598   \n",
       "\n",
       "       2522      490       1076  ...      1233      381       726       3186  \\\n",
       "0  0.483643  0.482178  0.429688  ...  0.000122  0.000114  0.000092  0.000092   \n",
       "\n",
       "       396       1226      285       1989      2419      427   \n",
       "0  0.000084  0.000057  0.000053  0.000038  0.000031  0.000029  \n",
       "\n",
       "[1 rows x 4096 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1539,  1338,  1919,  ...,   411,   385, 15899]])\n",
      "'metals , such as indium antimonide ( InSb ) and silver antimonide ( Ag \\n\\n\\n 3Sb ) . The alkali metal and zinc antimonides , such as Na3Sb and Zn3Sb2 , are more reactive . Treating these antimonides with acid produces the unstable gas stibine , SbH \\n\\n\\n 3 : \\n\\n\\n Sb3 − + 3 H + → SbH \\n\\n\\n 3 \\n\\n\\n Stibine can also be produced by treating Sb3 + salts with hydride reagents such as sodium borohydride.Stibine decomposes spontaneously at room temperature . Because stibine has a positive heat of formation , it is thermodynamically unstable and thus antimony does not react with hydrogen directly . \\n\\n\\n Organoantimony compounds are typically prepared by alkylation of antimony halides with Grignard reagents . A large variety of compounds are known with both Sb ( III ) and Sb ( V ) centers , including mixed chloro @-@ organic derivatives , anions , and cations . Examples include Sb ( C6H5 ) 3 ( triphenylstibine ) , Sb2 ( C6H5 ) 4 ( with an Sb @-@ Sb bond ) , and cyclic [ Sb ( C6H5 ) ] n . Pentacoordinated organoantimony compounds are common , examples being Sb ( C6H5 ) 5 and several related halides . \\n\\n\\n\\n\\n = = History = = \\n\\n\\n\\n\\n Antimony ( III ) sulfide , Sb2S3 , was recognized in predynastic Egypt as an eye cosmetic ( kohl ) as early as about 3100 BC , when the cosmetic palette was invented . \\n\\n\\n An artifact , said to be part of a vase , made of antimony dating to about 3000 BC was found at Telloh , Chaldea ( part of present @-@ day Iraq ) , and a copper object plated with antimony dating between 2500 BC and 2200 BC has been found in Egypt . Austen , at a lecture by Herbert Gladstone in 1892 commented that \" we only know of antimony at the present day as a highly brittle and crystalline metal , which could hardly be fashioned into a useful vase , and therefore this remarkable \\' find \\' ( artifact mentioned above ) must represent the lost art of rendering antimony malleable . \" \\n\\n\\n Moorey was unconvinced the artifact was indeed a vase , mentioning that Selimkhanov , after his analysis of the Tello object ( published in 1975 ) , \" attempted to relate the metal to Transcaucasian natural antimony \" ( i.e. native metal ) and that \" the antimony objects from Transcaucasia are all small personal ornaments . \" This weakens the evidence for a lost art \" of rendering antimony malleable . \" \\n\\n\\n The Roman scholar Pliny the Elder described several ways of preparing antimony sulfide for medical purposes in his treatise Natural History . Pliny the Elder also made a distinction between \" male \" and \" female \" forms of antimony ; the male form is probably the sulfide , while the female form , which is superior , heavier , and less friable , has been suspected to be native metallic antimony . \\n\\n\\n The Roman naturalist Pedanius Dioscorides mentioned that antimony sulfide could be roasted by heating by a current of air . It is thought that this produced metallic antimony . \\n\\n\\n The first description of a procedure for isolating antimony is in the book De la pirotechnia of 1540 by Vannoccio Biringuccio ; this predates the more famous 1556 book by Agricola , De re metallica . In this context Agricola has been often incorrectly credited with the discovery of metallic antimony . The book Currus Triumphalis Antimonii ( The Triumphal Chariot of Antimony ) , describing the preparation of metallic antimony , was published in Germany in 1604 . It was purported to have been written by a Benedictine monk , writing under the name Basilius Valentinus , in the 15th century ; if it were authentic , which it is not , it would predate Biringuccio . \\n\\n\\n The metal antimony was known to German chemist Andreas Libavius in 1615 who obtained it by adding iron to a molten mixture of antimony sulfide , salt and potassium tartrate . This procedure produced antimony with a crystalline or starred surface . \\n\\n\\n With the advent of challenges to phlogiston theory it was recognized that antimony is an element forming sulfides , oxides , and other compounds , as is the case with other metals . \\n\\n\\n The first natural occurrence of pure antimony in the Earth \\'s crust was described by the Swedish scientist and local mine district engineer Anton von Swab in 1783 ; the type @-@ sample was collected from the Sala Silver Mine in the Bergslagen mining district of Sala , Västmanland , Sweden . \\n\\n\\n\\n\\n = = = Etymology = = = \\n\\n\\n\\n\\n The ancient words for antimony mostly have , as their chief meaning , kohl , the sulfide of antimony . \\n\\n\\n The Egyptians called antimony mśdmt ; in hieroglyphs , the vowels are uncertain , but there is an Arabic tradition that the word is ميسديميت mesdemet . The Greek word , στίμμι stimmi , is probably a loan word from Arabic or from Egyptian stm \\n\\n\\n and is used by Attic tragic poets of the 5th century BC ; later Greeks also used στἰβι stibi , as did Celsus and Pliny , writing in Latin , in the first century AD . Pliny also gives the names stimi [ sic ] , larbaris , alabaster , and the \" very common \" platyophthalmos , \" wide @-@ eye \" ( from the effect of the cosmetic ) . Later Latin authors adapted the word to Latin as stibium . The Arabic word for the substance , as opposed to the cosmetic , can appear as إثمد ithmid , athmoud , othmod , or uthmod . Littré suggests the first form , which is the earliest , derives from stimmida , an accusative for stimmi . \\n\\n\\n The use of Sb as the standard chemical symbol for antimony is due to Jöns Jakob Berzelius , who used this abbreviation of the name stibium . The medieval Latin form , from which the modern languages and late Byzantine Greek take their names for antimony , is antimonium . The origin of this is uncertain ; all suggestions have some difficulty either of form or interpretation . The popular etymology , from ἀντίμοναχός anti @-@ monachos or French antimoine , still has adherents ; this would mean \" monk @-@ killer \" , and is explained by many early alchemists being monks , and antimony being poisonous . \\n\\n\\n Another popular etymology is the hypothetical Greek word ἀντίμόνος antimonos , \" against aloneness \" , explained as \" not found as metal \" , or \" not found unalloyed \" . Lippmann conjectured a hypothetical Greek word ανθήμόνιον anthemonion , which would mean \" floret \" , and cites several examples of related Greek words ( but not that one ) which describe chemical or biological efflorescence . \\n\\n\\n The early uses of antimonium include the translations , in 1050 – 1100 , by Constantine the African of Arabic medical treatises . Several authorities believe antimonium is a scribal corruption of some Arabic form ; Meyerhof derives it from ithmid ; other possibilities include athimar , the Arabic name of the metalloid , and a hypothetical as @-@ stimmi , derived from or parallel to the Greek . \\n\\n\\n\\n\\n = = Production = = \\n\\n\\n\\n\\n\\n\\n = = = Top producers and production volumes = = = \\n\\n\\n\\n\\n The British Geological Survey ( BGS ) reported that in 2005 , China was the top producer of antimony with an approximately 84 % world share , followed at a distance by South Africa , Bolivia and Tajikistan . Xikuangshan Mine in Hunan province has the largest deposits in China with an estimated'\n",
      "'<0x0A>'\n",
      "'3 \\n\\n'\n"
     ]
    }
   ],
   "source": [
    "layer_index = 2\n",
    "display(pd.DataFrame(layer_inps[layer_index, 0, :].abs()).sort_values(0, ascending=False).T)\n",
    "display(pd.DataFrame(layer_inps[layer_index, 113, :].abs()).sort_values(0, ascending=False).T)\n",
    "\n",
    "print(dataloader[0][0][0])\n",
    "print(repr(lm.tokenizer.decode(dataloader[0][0][0])))\n",
    "print(repr(lm.tokenizer.decode(dataloader[0][0][0][113])))\n",
    "print(repr(lm.tokenizer.decode(dataloader[0][0][0][111:115])))\n",
    "# print(repr(lm.tokenizer.decode(dataloader[52][0][0][225:229])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb96d317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2533</th>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.066895</td>\n",
       "      <td>170.375</td>\n",
       "      <td>170.0</td>\n",
       "      <td>167.875</td>\n",
       "      <td>166.875</td>\n",
       "      <td>165.375</td>\n",
       "      <td>165.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>165.375</td>\n",
       "      <td>...</td>\n",
       "      <td>195.5</td>\n",
       "      <td>202.25</td>\n",
       "      <td>201.125</td>\n",
       "      <td>196.25</td>\n",
       "      <td>195.0</td>\n",
       "      <td>193.75</td>\n",
       "      <td>195.375</td>\n",
       "      <td>194.625</td>\n",
       "      <td>182.25</td>\n",
       "      <td>35.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>0.005524</td>\n",
       "      <td>0.178467</td>\n",
       "      <td>1623.000</td>\n",
       "      <td>1626.0</td>\n",
       "      <td>1626.000</td>\n",
       "      <td>1631.000</td>\n",
       "      <td>1631.000</td>\n",
       "      <td>1631.0</td>\n",
       "      <td>1631.0</td>\n",
       "      <td>1631.000</td>\n",
       "      <td>...</td>\n",
       "      <td>1631.0</td>\n",
       "      <td>1631.00</td>\n",
       "      <td>1631.000</td>\n",
       "      <td>1631.00</td>\n",
       "      <td>1631.0</td>\n",
       "      <td>1631.00</td>\n",
       "      <td>1631.000</td>\n",
       "      <td>1631.000</td>\n",
       "      <td>1606.00</td>\n",
       "      <td>69.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1512</th>\n",
       "      <td>0.024292</td>\n",
       "      <td>0.981445</td>\n",
       "      <td>2754.000</td>\n",
       "      <td>2758.0</td>\n",
       "      <td>2758.000</td>\n",
       "      <td>2768.000</td>\n",
       "      <td>2768.000</td>\n",
       "      <td>2768.0</td>\n",
       "      <td>2768.0</td>\n",
       "      <td>2768.000</td>\n",
       "      <td>...</td>\n",
       "      <td>2772.0</td>\n",
       "      <td>2772.00</td>\n",
       "      <td>2772.000</td>\n",
       "      <td>2772.00</td>\n",
       "      <td>2772.0</td>\n",
       "      <td>2772.00</td>\n",
       "      <td>2772.000</td>\n",
       "      <td>2772.000</td>\n",
       "      <td>2724.00</td>\n",
       "      <td>138.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2       3         4         5         6   \\\n",
       "2533  0.000166  0.066895   170.375   170.0   167.875   166.875   165.375   \n",
       "1415  0.005524  0.178467  1623.000  1626.0  1626.000  1631.000  1631.000   \n",
       "1512  0.024292  0.981445  2754.000  2758.0  2758.000  2768.000  2768.000   \n",
       "\n",
       "          7       8         9   ...      22       23        24       25  \\\n",
       "2533   165.0   166.0   165.375  ...   195.5   202.25   201.125   196.25   \n",
       "1415  1631.0  1631.0  1631.000  ...  1631.0  1631.00  1631.000  1631.00   \n",
       "1512  2768.0  2768.0  2768.000  ...  2772.0  2772.00  2772.000  2772.00   \n",
       "\n",
       "          26       27        28        29       30       31  \n",
       "2533   195.0   193.75   195.375   194.625   182.25   35.625  \n",
       "1415  1631.0  1631.00  1631.000  1631.000  1606.00   69.000  \n",
       "1512  2772.0  2772.00  2772.000  2772.000  2724.00  138.000  \n",
       "\n",
       "[3 rows x 32 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(layer_inps[:, 113, (2533, 1415, 1512)].abs().sort().values, columns=[\"2533\", \"1415\", \"1512\"]).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df16f47",
   "metadata": {},
   "source": [
    "### outlier channel\n",
    "- layer에는 유독 높은 값들을 가지는 channel들이 존재함\n",
    "    - 여기서 channel은 token vector component의 index를 의미함\n",
    "        - 더 좋은 표현?\n",
    "- outlier channel의 대부분의 vector component들이 vector 내에서 이상치로 나타남"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0902aded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sqared Mean: tensor(15.5762) Standardized Sqared Mean: tensor(0.9995)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>113</th>\n",
       "      <th>0</th>\n",
       "      <th>872</th>\n",
       "      <th>2033</th>\n",
       "      <th>590</th>\n",
       "      <th>1072</th>\n",
       "      <th>2013</th>\n",
       "      <th>1567</th>\n",
       "      <th>905</th>\n",
       "      <th>1625</th>\n",
       "      <th>...</th>\n",
       "      <th>1850</th>\n",
       "      <th>102</th>\n",
       "      <th>29</th>\n",
       "      <th>713</th>\n",
       "      <th>1416</th>\n",
       "      <th>58</th>\n",
       "      <th>63</th>\n",
       "      <th>33</th>\n",
       "      <th>90</th>\n",
       "      <th>75</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>170.375</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1.896484</td>\n",
       "      <td>1.871094</td>\n",
       "      <td>1.859375</td>\n",
       "      <td>1.810547</td>\n",
       "      <td>1.772461</td>\n",
       "      <td>1.748047</td>\n",
       "      <td>1.720703</td>\n",
       "      <td>1.708008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137817</td>\n",
       "      <td>0.124756</td>\n",
       "      <td>0.115234</td>\n",
       "      <td>0.108276</td>\n",
       "      <td>0.101318</td>\n",
       "      <td>0.099609</td>\n",
       "      <td>0.080078</td>\n",
       "      <td>0.030457</td>\n",
       "      <td>0.016479</td>\n",
       "      <td>0.001953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 2048 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      113   0         872       2033      590       1072      2013      1567  \\\n",
       "0  170.375  39.0  1.896484  1.871094  1.859375  1.810547  1.772461  1.748047   \n",
       "\n",
       "       905       1625  ...      1850      102       29        713       1416  \\\n",
       "0  1.720703  1.708008  ...  0.137817  0.124756  0.115234  0.108276  0.101318   \n",
       "\n",
       "       58        63        33        90        75    \n",
       "0  0.099609  0.080078  0.030457  0.016479  0.001953  \n",
       "\n",
       "[1 rows x 2048 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sqared Mean: tensor(3980.9758) Standardized Sqared Mean: tensor(0.9995)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>113</th>\n",
       "      <th>0</th>\n",
       "      <th>1919</th>\n",
       "      <th>1891</th>\n",
       "      <th>1939</th>\n",
       "      <th>757</th>\n",
       "      <th>1938</th>\n",
       "      <th>1975</th>\n",
       "      <th>1720</th>\n",
       "      <th>1930</th>\n",
       "      <th>...</th>\n",
       "      <th>528</th>\n",
       "      <th>100</th>\n",
       "      <th>537</th>\n",
       "      <th>691</th>\n",
       "      <th>379</th>\n",
       "      <th>514</th>\n",
       "      <th>1998</th>\n",
       "      <th>118</th>\n",
       "      <th>1227</th>\n",
       "      <th>1732</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2754.0</td>\n",
       "      <td>754.0</td>\n",
       "      <td>0.202393</td>\n",
       "      <td>0.194214</td>\n",
       "      <td>0.177124</td>\n",
       "      <td>0.164307</td>\n",
       "      <td>0.151367</td>\n",
       "      <td>0.149658</td>\n",
       "      <td>0.148315</td>\n",
       "      <td>0.145386</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 2048 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     113    0         1919      1891      1939      757       1938      1975  \\\n",
       "0  2754.0  754.0  0.202393  0.194214  0.177124  0.164307  0.151367  0.149658   \n",
       "\n",
       "       1720      1930  ...      528       100       537       691       379   \\\n",
       "0  0.148315  0.145386  ...  0.000366  0.000366  0.000366  0.000351  0.000259   \n",
       "\n",
       "       514       1998      118       1227      1732  \n",
       "0  0.000229  0.000175  0.000137  0.000092  0.000092  \n",
       "\n",
       "[1 rows x 2048 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layer_inps_mean = layer_inps.mean(dim=2, keepdim=True)\n",
    "layer_inps_std = layer_inps.std(dim=2, keepdim=True)\n",
    "layer_inps_standardized = (layer_inps - layer_inps_mean) / layer_inps_std\n",
    "\n",
    "def display_channel(channel_index):\n",
    "    print(\"Sqared Mean:\", layer_inps[layer_index, :, channel_index].square().mean(dim=0), \"Standardized Sqared Mean:\", layer_inps_standardized[layer_index, :, channel_index].square().mean(dim=0))\n",
    "    display(pd.DataFrame(layer_inps[layer_index, :, channel_index].abs()).sort_values(0, ascending=False).T)\n",
    "    \n",
    "# 전체적으로 outlier인 channel\n",
    "display_channel(1512)\n",
    "# display_channel(2298)\n",
    "\n",
    "# 한두개의 component만 극단적으로 큰 channel\n",
    "display_channel(2533)\n",
    "# display_channel(1415)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0a336f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.350586</td>\n",
       "      <td>754.000000</td>\n",
       "      <td>758.500000</td>\n",
       "      <td>758.500000</td>\n",
       "      <td>766.000000</td>\n",
       "      <td>766.000000</td>\n",
       "      <td>766.000000</td>\n",
       "      <td>766.000000</td>\n",
       "      <td>766.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>772.000000</td>\n",
       "      <td>772.500000</td>\n",
       "      <td>773.000000</td>\n",
       "      <td>773.000000</td>\n",
       "      <td>772.500000</td>\n",
       "      <td>772.500000</td>\n",
       "      <td>772.000000</td>\n",
       "      <td>771.000000</td>\n",
       "      <td>765.500000</td>\n",
       "      <td>237.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005371</td>\n",
       "      <td>0.229492</td>\n",
       "      <td>0.116699</td>\n",
       "      <td>0.245605</td>\n",
       "      <td>0.364502</td>\n",
       "      <td>0.697754</td>\n",
       "      <td>1.172852</td>\n",
       "      <td>1.472656</td>\n",
       "      <td>1.678711</td>\n",
       "      <td>1.897461</td>\n",
       "      <td>...</td>\n",
       "      <td>14.664062</td>\n",
       "      <td>15.617188</td>\n",
       "      <td>16.203125</td>\n",
       "      <td>17.562500</td>\n",
       "      <td>17.906250</td>\n",
       "      <td>18.046875</td>\n",
       "      <td>17.421875</td>\n",
       "      <td>16.796875</td>\n",
       "      <td>15.578125</td>\n",
       "      <td>7.550781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003845</td>\n",
       "      <td>0.032684</td>\n",
       "      <td>0.009064</td>\n",
       "      <td>0.117798</td>\n",
       "      <td>0.263916</td>\n",
       "      <td>0.341309</td>\n",
       "      <td>0.636719</td>\n",
       "      <td>0.782227</td>\n",
       "      <td>1.051758</td>\n",
       "      <td>1.296875</td>\n",
       "      <td>...</td>\n",
       "      <td>10.750000</td>\n",
       "      <td>11.820312</td>\n",
       "      <td>12.515625</td>\n",
       "      <td>14.492188</td>\n",
       "      <td>15.414062</td>\n",
       "      <td>15.601562</td>\n",
       "      <td>14.773438</td>\n",
       "      <td>14.390625</td>\n",
       "      <td>13.437500</td>\n",
       "      <td>6.480469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.003334</td>\n",
       "      <td>0.040710</td>\n",
       "      <td>0.063477</td>\n",
       "      <td>0.186035</td>\n",
       "      <td>0.216309</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.708008</td>\n",
       "      <td>0.898926</td>\n",
       "      <td>1.162109</td>\n",
       "      <td>...</td>\n",
       "      <td>10.867188</td>\n",
       "      <td>11.929688</td>\n",
       "      <td>12.609375</td>\n",
       "      <td>13.257812</td>\n",
       "      <td>13.468750</td>\n",
       "      <td>14.414062</td>\n",
       "      <td>13.843750</td>\n",
       "      <td>13.406250</td>\n",
       "      <td>11.421875</td>\n",
       "      <td>5.359375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.078735</td>\n",
       "      <td>0.023499</td>\n",
       "      <td>0.078735</td>\n",
       "      <td>0.153320</td>\n",
       "      <td>0.194214</td>\n",
       "      <td>0.459961</td>\n",
       "      <td>0.582520</td>\n",
       "      <td>0.713867</td>\n",
       "      <td>0.931641</td>\n",
       "      <td>...</td>\n",
       "      <td>14.453125</td>\n",
       "      <td>15.742188</td>\n",
       "      <td>16.718750</td>\n",
       "      <td>19.062500</td>\n",
       "      <td>19.640625</td>\n",
       "      <td>20.390625</td>\n",
       "      <td>19.703125</td>\n",
       "      <td>19.156250</td>\n",
       "      <td>17.859375</td>\n",
       "      <td>9.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043</th>\n",
       "      <td>0.003113</td>\n",
       "      <td>0.093079</td>\n",
       "      <td>0.038757</td>\n",
       "      <td>0.030075</td>\n",
       "      <td>0.127319</td>\n",
       "      <td>0.118408</td>\n",
       "      <td>0.141235</td>\n",
       "      <td>0.105469</td>\n",
       "      <td>0.072083</td>\n",
       "      <td>0.166504</td>\n",
       "      <td>...</td>\n",
       "      <td>1.139648</td>\n",
       "      <td>0.812012</td>\n",
       "      <td>0.829590</td>\n",
       "      <td>0.790527</td>\n",
       "      <td>0.980469</td>\n",
       "      <td>0.925781</td>\n",
       "      <td>1.142578</td>\n",
       "      <td>1.537109</td>\n",
       "      <td>0.894531</td>\n",
       "      <td>2.628906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044</th>\n",
       "      <td>0.001167</td>\n",
       "      <td>0.089722</td>\n",
       "      <td>0.037445</td>\n",
       "      <td>0.044434</td>\n",
       "      <td>0.047546</td>\n",
       "      <td>0.007324</td>\n",
       "      <td>0.009613</td>\n",
       "      <td>0.077942</td>\n",
       "      <td>0.278564</td>\n",
       "      <td>0.247803</td>\n",
       "      <td>...</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.981445</td>\n",
       "      <td>1.129883</td>\n",
       "      <td>1.154297</td>\n",
       "      <td>1.148438</td>\n",
       "      <td>1.240234</td>\n",
       "      <td>1.701172</td>\n",
       "      <td>2.421875</td>\n",
       "      <td>1.971680</td>\n",
       "      <td>0.445312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2045</th>\n",
       "      <td>0.003021</td>\n",
       "      <td>0.097107</td>\n",
       "      <td>0.055481</td>\n",
       "      <td>0.066345</td>\n",
       "      <td>0.169678</td>\n",
       "      <td>0.245117</td>\n",
       "      <td>0.281494</td>\n",
       "      <td>0.339111</td>\n",
       "      <td>0.370605</td>\n",
       "      <td>0.608887</td>\n",
       "      <td>...</td>\n",
       "      <td>1.959961</td>\n",
       "      <td>3.033203</td>\n",
       "      <td>3.085938</td>\n",
       "      <td>4.164062</td>\n",
       "      <td>3.169922</td>\n",
       "      <td>3.929688</td>\n",
       "      <td>3.843750</td>\n",
       "      <td>3.589844</td>\n",
       "      <td>4.078125</td>\n",
       "      <td>4.351562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2046</th>\n",
       "      <td>0.000748</td>\n",
       "      <td>0.055847</td>\n",
       "      <td>0.097107</td>\n",
       "      <td>0.038208</td>\n",
       "      <td>0.132324</td>\n",
       "      <td>0.076660</td>\n",
       "      <td>0.156006</td>\n",
       "      <td>0.175659</td>\n",
       "      <td>0.269043</td>\n",
       "      <td>0.364990</td>\n",
       "      <td>...</td>\n",
       "      <td>3.126953</td>\n",
       "      <td>3.781250</td>\n",
       "      <td>4.875000</td>\n",
       "      <td>5.578125</td>\n",
       "      <td>4.480469</td>\n",
       "      <td>4.621094</td>\n",
       "      <td>4.261719</td>\n",
       "      <td>3.406250</td>\n",
       "      <td>2.953125</td>\n",
       "      <td>0.474609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2047</th>\n",
       "      <td>0.013000</td>\n",
       "      <td>0.077271</td>\n",
       "      <td>0.127441</td>\n",
       "      <td>0.053802</td>\n",
       "      <td>0.138062</td>\n",
       "      <td>0.174194</td>\n",
       "      <td>0.213989</td>\n",
       "      <td>0.322510</td>\n",
       "      <td>0.320557</td>\n",
       "      <td>0.462891</td>\n",
       "      <td>...</td>\n",
       "      <td>1.534180</td>\n",
       "      <td>1.562500</td>\n",
       "      <td>1.912109</td>\n",
       "      <td>2.074219</td>\n",
       "      <td>2.023438</td>\n",
       "      <td>1.594727</td>\n",
       "      <td>1.203125</td>\n",
       "      <td>0.642578</td>\n",
       "      <td>1.040039</td>\n",
       "      <td>1.256836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2048 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1           2           3           4           5   \\\n",
       "0     0.000759  0.350586  754.000000  758.500000  758.500000  766.000000   \n",
       "1     0.005371  0.229492    0.116699    0.245605    0.364502    0.697754   \n",
       "2     0.003845  0.032684    0.009064    0.117798    0.263916    0.341309   \n",
       "3     0.000197  0.003334    0.040710    0.063477    0.186035    0.216309   \n",
       "4     0.000759  0.078735    0.023499    0.078735    0.153320    0.194214   \n",
       "...        ...       ...         ...         ...         ...         ...   \n",
       "2043  0.003113  0.093079    0.038757    0.030075    0.127319    0.118408   \n",
       "2044  0.001167  0.089722    0.037445    0.044434    0.047546    0.007324   \n",
       "2045  0.003021  0.097107    0.055481    0.066345    0.169678    0.245117   \n",
       "2046  0.000748  0.055847    0.097107    0.038208    0.132324    0.076660   \n",
       "2047  0.013000  0.077271    0.127441    0.053802    0.138062    0.174194   \n",
       "\n",
       "              6           7           8           9   ...          22  \\\n",
       "0     766.000000  766.000000  766.000000  766.000000  ...  772.000000   \n",
       "1       1.172852    1.472656    1.678711    1.897461  ...   14.664062   \n",
       "2       0.636719    0.782227    1.051758    1.296875  ...   10.750000   \n",
       "3       0.468750    0.708008    0.898926    1.162109  ...   10.867188   \n",
       "4       0.459961    0.582520    0.713867    0.931641  ...   14.453125   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2043    0.141235    0.105469    0.072083    0.166504  ...    1.139648   \n",
       "2044    0.009613    0.077942    0.278564    0.247803  ...    0.953125   \n",
       "2045    0.281494    0.339111    0.370605    0.608887  ...    1.959961   \n",
       "2046    0.156006    0.175659    0.269043    0.364990  ...    3.126953   \n",
       "2047    0.213989    0.322510    0.320557    0.462891  ...    1.534180   \n",
       "\n",
       "              23          24          25          26          27          28  \\\n",
       "0     772.500000  773.000000  773.000000  772.500000  772.500000  772.000000   \n",
       "1      15.617188   16.203125   17.562500   17.906250   18.046875   17.421875   \n",
       "2      11.820312   12.515625   14.492188   15.414062   15.601562   14.773438   \n",
       "3      11.929688   12.609375   13.257812   13.468750   14.414062   13.843750   \n",
       "4      15.742188   16.718750   19.062500   19.640625   20.390625   19.703125   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2043    0.812012    0.829590    0.790527    0.980469    0.925781    1.142578   \n",
       "2044    0.981445    1.129883    1.154297    1.148438    1.240234    1.701172   \n",
       "2045    3.033203    3.085938    4.164062    3.169922    3.929688    3.843750   \n",
       "2046    3.781250    4.875000    5.578125    4.480469    4.621094    4.261719   \n",
       "2047    1.562500    1.912109    2.074219    2.023438    1.594727    1.203125   \n",
       "\n",
       "              29          30          31  \n",
       "0     771.000000  765.500000  237.000000  \n",
       "1      16.796875   15.578125    7.550781  \n",
       "2      14.390625   13.437500    6.480469  \n",
       "3      13.406250   11.421875    5.359375  \n",
       "4      19.156250   17.859375    9.781250  \n",
       "...          ...         ...         ...  \n",
       "2043    1.537109    0.894531    2.628906  \n",
       "2044    2.421875    1.971680    0.445312  \n",
       "2045    3.589844    4.078125    4.351562  \n",
       "2046    3.406250    2.953125    0.474609  \n",
       "2047    0.642578    1.040039    1.256836  \n",
       "\n",
       "[2048 rows x 32 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.350586</td>\n",
       "      <td>754.0</td>\n",
       "      <td>758.5</td>\n",
       "      <td>758.5</td>\n",
       "      <td>766.0</td>\n",
       "      <td>766.0</td>\n",
       "      <td>766.0</td>\n",
       "      <td>766.0</td>\n",
       "      <td>766.0</td>\n",
       "      <td>...</td>\n",
       "      <td>772.0</td>\n",
       "      <td>772.5</td>\n",
       "      <td>773.0</td>\n",
       "      <td>773.0</td>\n",
       "      <td>772.5</td>\n",
       "      <td>772.5</td>\n",
       "      <td>772.0</td>\n",
       "      <td>771.0</td>\n",
       "      <td>765.5</td>\n",
       "      <td>237.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005524</td>\n",
       "      <td>0.066895</td>\n",
       "      <td>2754.0</td>\n",
       "      <td>2758.0</td>\n",
       "      <td>2758.0</td>\n",
       "      <td>2768.0</td>\n",
       "      <td>2768.0</td>\n",
       "      <td>2768.0</td>\n",
       "      <td>2768.0</td>\n",
       "      <td>2768.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2772.0</td>\n",
       "      <td>2772.0</td>\n",
       "      <td>2772.0</td>\n",
       "      <td>2772.0</td>\n",
       "      <td>2772.0</td>\n",
       "      <td>2772.0</td>\n",
       "      <td>2772.0</td>\n",
       "      <td>2772.0</td>\n",
       "      <td>2724.0</td>\n",
       "      <td>138.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1       2       3       4       5       6       7       8   \\\n",
       "0  0.000759  0.350586   754.0   758.5   758.5   766.0   766.0   766.0   766.0   \n",
       "1  0.005524  0.066895  2754.0  2758.0  2758.0  2768.0  2768.0  2768.0  2768.0   \n",
       "\n",
       "       9   ...      22      23      24      25      26      27      28  \\\n",
       "0   766.0  ...   772.0   772.5   773.0   773.0   772.5   772.5   772.0   \n",
       "1  2768.0  ...  2772.0  2772.0  2772.0  2772.0  2772.0  2772.0  2772.0   \n",
       "\n",
       "       29      30     31  \n",
       "0   771.0   765.5  237.0  \n",
       "1  2772.0  2724.0  138.0  \n",
       "\n",
       "[2 rows x 32 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(layer_inps[:, :, 2533].abs()).T)\n",
    "display(pd.DataFrame(layer_inps[:, (0,113), 2533].abs()).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb6408d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2281</th>\n",
       "      <th>456</th>\n",
       "      <th>714</th>\n",
       "      <th>2582</th>\n",
       "      <th>1812</th>\n",
       "      <th>3919</th>\n",
       "      <th>1733</th>\n",
       "      <th>2667</th>\n",
       "      <th>1416</th>\n",
       "      <th>545</th>\n",
       "      <th>...</th>\n",
       "      <th>366</th>\n",
       "      <th>290</th>\n",
       "      <th>1151</th>\n",
       "      <th>3083</th>\n",
       "      <th>905</th>\n",
       "      <th>563</th>\n",
       "      <th>3640</th>\n",
       "      <th>3178</th>\n",
       "      <th>3568</th>\n",
       "      <th>2100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>std_magnitude</th>\n",
       "      <td>0.999512</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999511</td>\n",
       "      <td>0.999511</td>\n",
       "      <td>0.999511</td>\n",
       "      <td>0.999511</td>\n",
       "      <td>0.999511</td>\n",
       "      <td>0.999511</td>\n",
       "      <td>0.999511</td>\n",
       "      <td>0.999511</td>\n",
       "      <td>0.999511</td>\n",
       "      <td>0.999511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>magnitude</th>\n",
       "      <td>0.017211</td>\n",
       "      <td>0.003044</td>\n",
       "      <td>0.003690</td>\n",
       "      <td>0.009358</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.004441</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>0.002723</td>\n",
       "      <td>0.001829</td>\n",
       "      <td>0.004165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002896</td>\n",
       "      <td>0.004974</td>\n",
       "      <td>0.005833</td>\n",
       "      <td>0.001916</td>\n",
       "      <td>0.003003</td>\n",
       "      <td>0.003331</td>\n",
       "      <td>0.003710</td>\n",
       "      <td>0.006088</td>\n",
       "      <td>0.003830</td>\n",
       "      <td>0.001646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 4096 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   2281      456       714       2582      1812      3919  \\\n",
       "std_magnitude  0.999512  0.999512  0.999512  0.999512  0.999512  0.999512   \n",
       "magnitude      0.017211  0.003044  0.003690  0.009358  0.002119  0.004441   \n",
       "\n",
       "                   1733      2667      1416      545   ...      366   \\\n",
       "std_magnitude  0.999512  0.999512  0.999512  0.999512  ...  0.999511   \n",
       "magnitude      0.001782  0.002723  0.001829  0.004165  ...  0.002896   \n",
       "\n",
       "                   290       1151      3083      905       563       3640  \\\n",
       "std_magnitude  0.999511  0.999511  0.999511  0.999511  0.999511  0.999511   \n",
       "magnitude      0.004974  0.005833  0.001916  0.003003  0.003331  0.003710   \n",
       "\n",
       "                   3178      3568      2100  \n",
       "std_magnitude  0.999511  0.999511  0.999511  \n",
       "magnitude      0.006088  0.003830  0.001646  \n",
       "\n",
       "[2 rows x 4096 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2533</th>\n",
       "      <th>1415</th>\n",
       "      <th>1512</th>\n",
       "      <th>2298</th>\n",
       "      <th>3431</th>\n",
       "      <th>2168</th>\n",
       "      <th>3135</th>\n",
       "      <th>339</th>\n",
       "      <th>2393</th>\n",
       "      <th>1076</th>\n",
       "      <th>...</th>\n",
       "      <th>3703</th>\n",
       "      <th>2319</th>\n",
       "      <th>2451</th>\n",
       "      <th>170</th>\n",
       "      <th>3923</th>\n",
       "      <th>1559</th>\n",
       "      <th>1301</th>\n",
       "      <th>3520</th>\n",
       "      <th>1951</th>\n",
       "      <th>1744</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>std_magnitude</th>\n",
       "      <td>0.999512</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>0.999512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>magnitude</th>\n",
       "      <td>3980.975342</td>\n",
       "      <td>1384.744507</td>\n",
       "      <td>15.576243</td>\n",
       "      <td>5.024303</td>\n",
       "      <td>2.137006</td>\n",
       "      <td>1.902065</td>\n",
       "      <td>1.855412</td>\n",
       "      <td>1.798153</td>\n",
       "      <td>1.525816</td>\n",
       "      <td>1.072778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>0.000684</td>\n",
       "      <td>0.000674</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.000659</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>0.000640</td>\n",
       "      <td>0.000635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 4096 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      2533         1415       1512      2298      3431  \\\n",
       "std_magnitude     0.999512     0.999512   0.999512  0.999512  0.999512   \n",
       "magnitude      3980.975342  1384.744507  15.576243  5.024303  2.137006   \n",
       "\n",
       "                   2168      3135      339       2393      1076  ...  \\\n",
       "std_magnitude  0.999512  0.999512  0.999512  0.999512  0.999512  ...   \n",
       "magnitude      1.902065  1.855412  1.798153  1.525816  1.072778  ...   \n",
       "\n",
       "                   3703      2319      2451      170       3923      1559  \\\n",
       "std_magnitude  0.999512  0.999512  0.999512  0.999512  0.999512  0.999512   \n",
       "magnitude      0.000691  0.000690  0.000686  0.000684  0.000674  0.000666   \n",
       "\n",
       "                   1301      3520      1951      1744  \n",
       "std_magnitude  0.999512  0.999512  0.999512  0.999512  \n",
       "magnitude      0.000659  0.000654  0.000640  0.000635  \n",
       "\n",
       "[2 rows x 4096 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layer_inps_mean = layer_inps.mean(dim=2, keepdim=True)\n",
    "layer_inps_std = layer_inps.std(dim=2, keepdim=True)\n",
    "layer_inps_standardized = (layer_inps - layer_inps_mean) / layer_inps_std\n",
    "\n",
    "layer_index = 2\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"std_magnitude\"] = layer_inps_standardized.square().mean(dim=1)[layer_index]\n",
    "df[\"magnitude\"] = layer_inps.square().mean(dim=1)[layer_index]\n",
    "\n",
    "display(df.sort_values(\"std_magnitude\", ascending=False).T)\n",
    "display(df.sort_values(\"magnitude\", ascending=False).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54adf46",
   "metadata": {},
   "source": [
    "## Todo\n",
    "- activation outlier 관련 논문 보기\n",
    "    - https://arxiv.org/abs/2309.15531\n",
    "    - https://arxiv.org/abs/2310.08041\n",
    "\n",
    "- decoder layer의 각 컴포넌트별로 outlier 확인하기\n",
    "  - 실제로 코딩하려면 decoder 내부 컴포넌트끼리의 연산에 적용시켜야 할 것 같음\n",
    "\n",
    "- outlier token과 outlier channel을 구분하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b032a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def omit_outliers1(values: torch.Tensor, *, threshold: float=0.1):\n",
    "    mean_prev = None\n",
    "    index = 0\n",
    "    for i in range(values.shape[0]):\n",
    "        mean = values[i:].mean(dim=0)\n",
    "        if mean_prev is not None: \n",
    "            # print(mean_prev - mean)\n",
    "            if mean_prev - mean > threshold:\n",
    "                index = i+1\n",
    "                break\n",
    "            \n",
    "        mean_prev = mean\n",
    "    return index\n",
    "\n",
    "def omit_outliers2(values: torch.Tensor, *, threshold: float=0.1):\n",
    "    mean_prev = None\n",
    "    index = 0\n",
    "    for i in range(values.shape[0]):\n",
    "        sum = values[:i+1].sum(dim=0)\n",
    "        mean = sum / (i+1)\n",
    "        if mean_prev is not None: \n",
    "            if (mean_prev-mean).abs() > threshold:\n",
    "                index = i+1\n",
    "                break\n",
    "            \n",
    "        mean_prev = mean\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c04208ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 2 tensor([113,   0])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2036</th>\n",
       "      <th>2037</th>\n",
       "      <th>2038</th>\n",
       "      <th>2039</th>\n",
       "      <th>2040</th>\n",
       "      <th>2041</th>\n",
       "      <th>2042</th>\n",
       "      <th>2043</th>\n",
       "      <th>2044</th>\n",
       "      <th>2045</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.896484</td>\n",
       "      <td>1.871094</td>\n",
       "      <td>1.859375</td>\n",
       "      <td>1.810547</td>\n",
       "      <td>1.772461</td>\n",
       "      <td>1.748047</td>\n",
       "      <td>1.720703</td>\n",
       "      <td>1.708008</td>\n",
       "      <td>1.692383</td>\n",
       "      <td>1.675781</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137817</td>\n",
       "      <td>0.124756</td>\n",
       "      <td>0.115234</td>\n",
       "      <td>0.108276</td>\n",
       "      <td>0.101318</td>\n",
       "      <td>0.099609</td>\n",
       "      <td>0.080078</td>\n",
       "      <td>0.030457</td>\n",
       "      <td>0.016479</td>\n",
       "      <td>0.001953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 2046 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6     \\\n",
       "0  1.896484  1.871094  1.859375  1.810547  1.772461  1.748047  1.720703   \n",
       "\n",
       "       7         8         9     ...      2036      2037      2038      2039  \\\n",
       "0  1.708008  1.692383  1.675781  ...  0.137817  0.124756  0.115234  0.108276   \n",
       "\n",
       "       2040      2041      2042      2043      2044      2045  \n",
       "0  0.101318  0.099609  0.080078  0.030457  0.016479  0.001953  \n",
       "\n",
       "[1 rows x 2046 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layer_index = 2\n",
    "# 1512, 2298, 1076\n",
    "# 2533, 1415\n",
    "outlier_channel = layer_inps[layer_index, :, 1512].abs().sort(dim=0, descending=True)\n",
    "index = omit_outliers2(outlier_channel.values)\n",
    "outlier_indices = outlier_channel.indices[:index]\n",
    "print(\"Index:\", index, outlier_indices)\n",
    "display(pd.DataFrame(outlier_channel.values[index:]).T)\n",
    "\n",
    "# outlier_channel.values[index:]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6529c774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 2 tensor([1512, 2298])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4086</th>\n",
       "      <th>4087</th>\n",
       "      <th>4088</th>\n",
       "      <th>4089</th>\n",
       "      <th>4090</th>\n",
       "      <th>4091</th>\n",
       "      <th>4092</th>\n",
       "      <th>4093</th>\n",
       "      <th>4094</th>\n",
       "      <th>4095</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tensor(18.2895)</td>\n",
       "      <td>tensor(13.6530)</td>\n",
       "      <td>tensor(10.2918)</td>\n",
       "      <td>tensor(8.3807)</td>\n",
       "      <td>tensor(8.2705)</td>\n",
       "      <td>tensor(7.9027)</td>\n",
       "      <td>tensor(7.0174)</td>\n",
       "      <td>tensor(6.2910)</td>\n",
       "      <td>tensor(6.2295)</td>\n",
       "      <td>tensor(6.0052)</td>\n",
       "      <td>...</td>\n",
       "      <td>tensor(0.4510)</td>\n",
       "      <td>tensor(0.4496)</td>\n",
       "      <td>tensor(0.4488)</td>\n",
       "      <td>tensor(0.4476)</td>\n",
       "      <td>tensor(0.4474)</td>\n",
       "      <td>tensor(0.4434)</td>\n",
       "      <td>tensor(0.4422)</td>\n",
       "      <td>tensor(0.4401)</td>\n",
       "      <td>tensor(0.4401)</td>\n",
       "      <td>tensor(0.4384)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tensor(1512)</td>\n",
       "      <td>tensor(2298)</td>\n",
       "      <td>tensor(1076)</td>\n",
       "      <td>tensor(3431)</td>\n",
       "      <td>tensor(2393)</td>\n",
       "      <td>tensor(3135)</td>\n",
       "      <td>tensor(257)</td>\n",
       "      <td>tensor(339)</td>\n",
       "      <td>tensor(490)</td>\n",
       "      <td>tensor(2789)</td>\n",
       "      <td>...</td>\n",
       "      <td>tensor(2127)</td>\n",
       "      <td>tensor(3156)</td>\n",
       "      <td>tensor(473)</td>\n",
       "      <td>tensor(3262)</td>\n",
       "      <td>tensor(1758)</td>\n",
       "      <td>tensor(752)</td>\n",
       "      <td>tensor(3759)</td>\n",
       "      <td>tensor(128)</td>\n",
       "      <td>tensor(1951)</td>\n",
       "      <td>tensor(2715)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 4096 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0                1                2               3     \\\n",
       "0  tensor(18.2895)  tensor(13.6530)  tensor(10.2918)  tensor(8.3807)   \n",
       "1     tensor(1512)     tensor(2298)     tensor(1076)    tensor(3431)   \n",
       "\n",
       "             4               5               6               7     \\\n",
       "0  tensor(8.2705)  tensor(7.9027)  tensor(7.0174)  tensor(6.2910)   \n",
       "1    tensor(2393)    tensor(3135)     tensor(257)     tensor(339)   \n",
       "\n",
       "             8               9     ...            4086            4087  \\\n",
       "0  tensor(6.2295)  tensor(6.0052)  ...  tensor(0.4510)  tensor(0.4496)   \n",
       "1     tensor(490)    tensor(2789)  ...    tensor(2127)    tensor(3156)   \n",
       "\n",
       "             4088            4089            4090            4091  \\\n",
       "0  tensor(0.4488)  tensor(0.4476)  tensor(0.4474)  tensor(0.4434)   \n",
       "1     tensor(473)    tensor(3262)    tensor(1758)     tensor(752)   \n",
       "\n",
       "             4092            4093            4094            4095  \n",
       "0  tensor(0.4422)  tensor(0.4401)  tensor(0.4401)  tensor(0.4384)  \n",
       "1    tensor(3759)     tensor(128)    tensor(1951)    tensor(2715)  \n",
       "\n",
       "[2 rows x 4096 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layer_index = 0\n",
    "channel_mags = layer_inps_standardized.abs().mean(dim=1)[layer_index].sort(dim=0, descending=True)\n",
    "index = omit_outliers2(channel_mags.values)\n",
    "outlier_indices = channel_mags.indices[:index]\n",
    "print(\"Index:\", index, outlier_indices)\n",
    "display(pd.DataFrame(channel_mags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed573576",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(layer_inps[2], \"layer_inps.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280d5907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0840, -0.7900,  0.0535,  ..., -0.1202, -0.1425,  0.0779],\n",
       "        [ 0.0284,  0.0437,  0.0077,  ..., -0.0048,  0.0084,  0.0276],\n",
       "        [-0.0130,  0.0528,  0.0924,  ...,  0.0028,  0.0343,  0.0456],\n",
       "        ...,\n",
       "        [-0.0132,  0.0287,  0.0182,  ...,  0.0069,  0.0234,  0.0186],\n",
       "        [ 0.0070,  0.0368, -0.0491,  ...,  0.0278, -0.0135, -0.0294],\n",
       "        [ 0.0287,  0.0312, -0.0296,  ...,  0.0105,  0.0010, -0.0268]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = torch.load(\"layer_inps.pt\")\n",
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2eb8d599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18.2895, 13.6530, 10.2918,  ...,  0.4401,  0.4401,  0.4384])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([87702512.0000, 850016.7500, 29490.6387,  ...,     1.5529,     1.5528,\n",
       "            1.5502])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([320388.1250, 12881.4854,  1253.5712,  ...,     1.3567,     1.3567,\n",
       "            1.3551])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([3.9596, 3.6846, 3.4241,  ..., 1.3647, 1.3647, 1.3635])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([  386.9820,   254.9887,   121.0999,  ...,     0.3317,     0.3310,\n",
       "            0.3245])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([   inf,    inf,    inf,  ..., 1.3933, 1.3924, 1.3834])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([       inf,        inf, 2849033632070193582856577357929512960.0000,  ...,     1.2585,     1.2579,\n",
       "            1.2523])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([6.9610, 6.5451, 5.8048,  ..., 1.2864, 1.2859, 1.2811])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "d = (\n",
    "    layer_inps_standardized.abs()\n",
    "    .mean(dim=1)[layer_index]\n",
    "    .sort(dim=0, descending=True)\n",
    "    .values\n",
    ")\n",
    "display(d, d.exp(), d.exp2(), (d+1).log()+1)\n",
    "plt.plot(d[:10].cpu().numpy())\n",
    "plt.show()\n",
    "\n",
    "d = (\n",
    "    layer_inps_standardized.square()\n",
    "    .mean(dim=1)[layer_index]\n",
    "    .sort(dim=0, descending=True)\n",
    "    .values\n",
    ")\n",
    "\n",
    "display(d, d.exp(), d.exp2(), (d+1).log()+1)\n",
    "plt.plot(d[:10].cpu().numpy())\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
