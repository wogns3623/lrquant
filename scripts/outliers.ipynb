{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a01c5c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jhkim/workspace/lrquant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhkim/workspace/lrquant/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a720990f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import typing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.style as mplstyle\n",
    "\n",
    "mplstyle.use(\"fast\")\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "if \"CUDA_VISIBLE_DEVICES\" in os.environ:\n",
    "    print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "\n",
    "torch.set_printoptions(precision=4, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3772ac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.LMClass import LMClass\n",
    "\n",
    "lm = LMClass(\"./huggingface/llama-2-7b-hf\")\n",
    "\n",
    "\n",
    "from datautils import get_loaders\n",
    "\n",
    "dataloader, _ = get_loaders(\n",
    "    name=\"wikitext2\",\n",
    "    nsamples=128,\n",
    "    seed=2,\n",
    "    model=lm.model_name,\n",
    "    seqlen=lm.seqlen,\n",
    "    cache_dir=f\"./cache/wikitext2_128_2/dataloader.cache\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a18f202",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 47.54 GiB of which 25.06 MiB is free. Process 174438 has 28.76 GiB memory in use. Process 177140 has 15.11 GiB memory in use. Including non-PyTorch memory, this process has 3.62 GiB memory in use. Of the allocated memory 3.37 GiB is allocated by PyTorch, and 1.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     14\u001b[0m     lm\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 15\u001b[0m     \u001b[43mlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     lm\u001b[38;5;241m.\u001b[39mmodel(dataloader[batch_index][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(lm\u001b[38;5;241m.\u001b[39mdevice))\n",
      "File \u001b[0;32m~/workspace/lrquant/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:1900\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1896\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`.to` is not supported for `4-bit` or `8-bit` models. Please use the model as it is, since the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1897\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1898\u001b[0m     )\n\u001b[1;32m   1899\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1900\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/lrquant/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/lrquant/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/lrquant/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 802 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/workspace/lrquant/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/lrquant/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/workspace/lrquant/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 47.54 GiB of which 25.06 MiB is free. Process 174438 has 28.76 GiB memory in use. Process 177140 has 15.11 GiB memory in use. Including non-PyTorch memory, this process has 3.62 GiB memory in use. Of the allocated memory 3.37 GiB is allocated by PyTorch, and 1.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "layer_inps = torch.zeros((32, 2048, 4096), dtype=torch.float32)\n",
    "\n",
    "for i, layer in enumerate(lm.model.model.layers):\n",
    "\n",
    "    def layer_hook(layer_index):\n",
    "        def _fn(module, input: tuple[torch.Tensor], output):\n",
    "            layer_inps[layer_index] = input[0][0].detach().clone()\n",
    "\n",
    "        return _fn\n",
    "\n",
    "    layer._forward_hooks.clear()\n",
    "    layer.register_forward_hook(layer_hook(i))\n",
    "\n",
    "batch_index = 32\n",
    "with torch.no_grad():\n",
    "    lm.model.eval()\n",
    "    lm.model.to(lm.device)\n",
    "    lm.model(dataloader[batch_index][0].to(lm.device))\n",
    "    lm.model.cpu()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba683aa4",
   "metadata": {},
   "source": [
    "## 두 가지 종류의 outlier\n",
    "### outlier token \n",
    "- 전체 input sequence에서 유독 높은 vector magnitude들을 가지는 sequence가 있음\n",
    "- 대부분의 입력 데이터에서 0번 sequence의 token vector magnitude가 큰 값을 가짐\n",
    "    - <bos> 토큰?\n",
    "- 적어도 llama-2-7b 모델에서는 하나 이상의 0번이 아닌 outlier token이 존재함\n",
    "    - 몇 가지 입력 데이터를 확인해본 결과 \".\"이거나 \"\\n\" 토큰이었음\n",
    "- 이 outlier token은 여러 outlier component들을 가지지만, 대부분의 components들은 여전히 0에 가까운 값을 가짐\n",
    "    - 몇 개의 극단적인 outlier component들이 존재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc25049b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layer_inps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m display(pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mlayer_inps\u001b[49m[\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39msort(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, descending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)))\n\u001b[1;32m      2\u001b[0m display(pd\u001b[38;5;241m.\u001b[39mDataFrame(layer_inps[\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m30\u001b[39m]\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39msort(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, descending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mrepr\u001b[39m(lm\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode(dataloader[batch_index][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m30\u001b[39m])))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'layer_inps' is not defined"
     ]
    }
   ],
   "source": [
    "layer_index = 2\n",
    "display(pd.DataFrame(layer_inps[layer_index, 0].abs()).sort_values(0, ascending=False).T)\n",
    "display(pd.DataFrame(layer_inps[layer_index, 30].abs()).sort_values(0, ascending=False).T)\n",
    "\n",
    "print(repr(lm.tokenizer.decode(dataloader[batch_index][0][0][30])))\n",
    "print(repr(lm.tokenizer.decode(dataloader[batch_index][0][0][28:32])))\n",
    "print(repr(lm.tokenizer.decode(dataloader[52][0][0][225:229])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb96d317",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layer_inps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m display(pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mlayer_inps\u001b[49m[:, \u001b[38;5;241m30\u001b[39m, (\u001b[38;5;241m2533\u001b[39m, \u001b[38;5;241m1415\u001b[39m, \u001b[38;5;241m1512\u001b[39m)]\u001b[38;5;241m.\u001b[39mabs(), columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2533\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1415\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1512\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'layer_inps' is not defined"
     ]
    }
   ],
   "source": [
    "display(pd.DataFrame(layer_inps[:, 30, (2533, 1415, 1512)].abs(), columns=[\"2533\", \"1415\", \"1512\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f0be796",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layer_inps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m magnitudes \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_inps\u001b[49m\u001b[38;5;241m.\u001b[39msquare()\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# [32, 2048]\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(magnitudes\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      3\u001b[0m topk \u001b[38;5;241m=\u001b[39m magnitudes\u001b[38;5;241m.\u001b[39mtopk(\u001b[38;5;241m5\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'layer_inps' is not defined"
     ]
    }
   ],
   "source": [
    "magnitudes = layer_inps.square().mean(dim=2)  # [32, 2048]\n",
    "print(magnitudes.shape)\n",
    "topk = magnitudes.topk(5, dim=1)\n",
    "display(pd.DataFrame([list(zip(topk.values[i], topk.indices[i])) for i in range(32)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df16f47",
   "metadata": {},
   "source": [
    "### outlier channel\n",
    "- layer에는 유독 높은 값들을 가지는 channel들이 존재함\n",
    "    - 여기서 channel은 token vector component의 index를 의미함\n",
    "        - 더 좋은 표현?\n",
    "- outlier channel의 대부분의 vector component들이 vector 내에서 이상치로 나타남"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0902aded",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_inps_mean = layer_inps.mean(dim=2, keepdim=True)\n",
    "layer_inps_std = layer_inps.std(dim=2, keepdim=True)\n",
    "layer_inps_standardized = (layer_inps - layer_inps_mean) / layer_inps_std\n",
    "\n",
    "def display_channel(channel_index):\n",
    "    print(\"Sqared Mean:\", layer_inps[layer_index, :, channel_index].square().mean(dim=0), \"Standardized Sqared Mean:\", layer_inps_standardized[layer_index, :, channel_index].square().mean(dim=0))\n",
    "    display(pd.DataFrame(layer_inps[layer_index, :, channel_index].abs()).sort_values(0, ascending=False).T)\n",
    "    \n",
    "# 전체적으로 outlier인 channel\n",
    "display_channel(1512)\n",
    "# display_channel(2298)\n",
    "\n",
    "# 한두개의 component만 극단적으로 큰 channel\n",
    "display_channel(2533)\n",
    "# display_channel(1415)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb6408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"std_magnitude\"] = layer_inps_standardized[layer_index].square().mean(dim=0)\n",
    "df[\"magnitude\"] = layer_inps[layer_index].square().mean(dim=0)\n",
    "\n",
    "display(df.sort_values(\"std_magnitude\", ascending=False).T)\n",
    "display(df.sort_values(\"magnitude\", ascending=False).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54adf46",
   "metadata": {},
   "source": [
    "## Todo\n",
    "- activation outlier 관련 논문 보기\n",
    "    - https://arxiv.org/abs/2309.15531\n",
    "    - https://arxiv.org/abs/2310.08041\n",
    "\n",
    "- decoder layer의 각 컴포넌트별로 outlier 확인하기\n",
    "  - 실제로 코딩하려면 decoder 내부 컴포넌트끼리의 연산에 적용시켜야 할 것 같음\n",
    "\n",
    "- outlier token과 outlier channel을 구분하는 방법"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
