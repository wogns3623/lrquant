{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22776fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoTokenizer, LlamaForCausalLM\n",
    "import os\n",
    "import gc\n",
    "\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "\n",
    "# Load the model and tokenizer\n",
    "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model_path = \"../huggingface/llama-2-7b-hf\"\n",
    "model_path = \"../output/base1-lr.25\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da9d7874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f1ff57cbfa4fb1b30eef6aaaae3233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../output/base1-lr.25 were not used when initializing LlamaForCausalLM: ['model.layers.22.self_attn.q_proj.weight_quantizer.zeros', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.1.self_attn.v_proj.weight_quantizer.zeros', 'model.layers.21.post_attention_layernorm.bias', 'model.layers.4.self_attn.v_proj.weight_quantizer.zeros', 'model.layers.28.self_attn.o_proj.weight_quantizer.zeros', 'model.layers.28.mlp.gate_proj.weight_quantizer.zeros', 'model.layers.17.self_attn.q_proj.weight_quantizer.scales', 'model.layers.7.mlp.gate_proj.weight_quantizer.scales', 'model.layers.24.self_attn.v_proj.weight_quantizer.zeros', 'model.layers.3.mlp.up_proj.weight_quantizer.zeros', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.12.qkt_smooth_scale', 'model.layers.11.qkt_smooth_scale', 'model.layers.30.mlp.down_proj.weight_quantizer.scales', 'model.layers.7.self_attn.k_proj.weight_quantizer.scales', 'model.layers.31.mlp.gate_proj.weight_quantizer.zeros', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.2.mlp.up_proj.weight_quantizer.zeros', 'model.layers.11.mlp.up_proj.bias', 'model.layers.22.self_attn.k_proj.weight_quantizer.zeros', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.13.self_attn.q_proj.weight_quantizer.scales', 'model.layers.7.mlp.up_proj.weight_quantizer.scales', 'model.layers.15.self_attn.k_proj.weight_quantizer.scales', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.22.input_layernorm.bias', 'model.layers.8.mlp.gate_proj.weight_quantizer.zeros', 'model.layers.27.mlp.down_proj.weight_quantizer.scales', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.25.self_attn.o_proj.weight_quantizer.scales', 'model.layers.6.self_attn.k_proj.weight_quantizer.scales', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.9.self_attn.v_proj.weight_quantizer.scales', 'model.layers.6.qkt_smooth_scale', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.21.qkt_smooth_scale', 'model.layers.13.mlp.up_proj.weight_quantizer.scales', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.23.self_attn.q_proj.weight_quantizer.scales', 'model.layers.9.self_attn.o_proj.weight_quantizer.zeros', 'model.layers.28.mlp.down_proj.weight_quantizer.scales', 'model.layers.27.input_layernorm.bias', 'model.layers.12.mlp.down_proj.weight_quantizer.scales', 'model.layers.12.self_attn.o_proj.weight_quantizer.scales', 'model.layers.2.mlp.gate_proj.weight_quantizer.scales', 'model.layers.23.mlp.down_proj.weight_quantizer.scales', 'model.layers.18.mlp.up_proj.weight_quantizer.scales', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.24.self_attn.v_proj.weight_quantizer.scales', 'model.layers.25.self_attn.q_proj.weight_quantizer.scales', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.9.input_layernorm.bias', 'model.layers.9.self_attn.q_proj.weight_quantizer.zeros', 'model.layers.0.mlp.up_proj.bias', 'model.layers.12.self_attn.o_proj.weight_quantizer.zeros', 'model.layers.8.self_attn.q_proj.weight_quantizer.scales', 'model.layers.6.mlp.down_proj.weight_quantizer.scales', 'model.layers.11.mlp.up_proj.weight_quantizer.scales', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.25.self_attn.v_proj.weight_quantizer.zeros', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.1.mlp.gate_proj.weight_quantizer.zeros', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.26.mlp.gate_proj.weight_quantizer.zeros', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.8.post_attention_layernorm.bias', 'model.layers.10.self_attn.o_proj.weight_quantizer.scales', 'model.layers.17.self_attn.v_proj.weight_quantizer.scales', 'model.layers.1.self_attn.k_proj.weight_quantizer.zeros', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.21.self_attn.k_proj.weight_quantizer.zeros', 'model.layers.4.post_attention_layernorm.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.24.qkt_smooth_scale', 'model.layers.31.self_attn.v_proj.weight_quantizer.zeros', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.17.self_attn.k_proj.weight_quantizer.zeros', 'model.layers.22.mlp.down_proj.weight_quantizer.zeros', 'model.layers.4.self_attn.q_proj.weight_quantizer.zeros', 'model.layers.16.mlp.up_proj.weight_quantizer.scales', 'model.layers.8.mlp.down_proj.weight_quantizer.zeros', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.21.mlp.gate_proj.weight_quantizer.zeros', 'model.layers.7.mlp.up_proj.weight_quantizer.zeros', 'model.layers.18.post_attention_layernorm.bias', 'model.layers.5.self_attn.v_proj.weight_quantizer.scales', 'model.layers.0.qkt_smooth_scale', 'model.layers.7.self_attn.k_proj.weight_quantizer.zeros', 'model.layers.23.post_attention_layernorm.bias', 'model.layers.16.post_attention_layernorm.bias', 'model.layers.28.self_attn.k_proj.weight_quantizer.scales', 'model.layers.19.mlp.down_proj.weight_quantizer.scales', 'model.layers.7.self_attn.q_proj.weight_quantizer.scales', 'model.layers.15.self_attn.o_proj.weight_quantizer.zeros', 'model.layers.31.self_attn.k_proj.weight_quantizer.zeros', 'model.layers.10.self_attn.o_proj.weight_quantizer.zeros', 'model.layers.15.input_layernorm.bias', 'model.layers.30.mlp.up_proj.weight_quantizer.scales', 'model.layers.0.mlp.down_proj.weight_quantizer.zeros', 'model.layers.22.mlp.down_proj.weight_quantizer.scales', 'model.layers.15.self_attn.k_proj.weight_quantizer.zeros', 'model.layers.10.self_attn.v_proj.weight_quantizer.scales', 'model.layers.19.self_attn.q_proj.weight_quantizer.scales', 'model.layers.22.self_attn.q_proj.weight_quantizer.scales', 'model.layers.9.post_attention_layernorm.bias', 'model.layers.30.self_attn.k_proj.weight_quantizer.scales', 'model.layers.11.self_attn.v_proj.weight_quantizer.scales', 'model.layers.15.self_attn.v_proj.weight_quantizer.zeros', 'model.layers.18.input_layernorm.bias', 'model.layers.18.mlp.down_proj.weight_quantizer.scales', 'model.layers.19.mlp.up_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.14.mlp.up_proj.weight_quantizer.scales', 'model.layers.21.self_attn.v_proj.weight_quantizer.scales', 'model.layers.6.mlp.up_proj.weight_quantizer.scales', 'model.layers.18.self_attn.q_proj.weight_quantizer.zeros', 'model.layers.14.mlp.up_proj.weight_quantizer.zeros', 'model.layers.16.qkt_smooth_scale', 'model.layers.2.post_attention_layernorm.bias', 'model.layers.9.self_attn.v_proj.weight_quantizer.zeros', 'model.layers.16.self_attn.o_proj.weight_quantizer.zeros', 'model.layers.11.post_attention_layernorm.bias', 'model.layers.4.self_attn.o_proj.weight_quantizer.zeros', 'model.layers.26.self_attn.k_proj.weight_quantizer.scales', 'model.layers.27.post_attention_layernorm.bias', 'model.layers.28.self_attn.o_proj.weight_quantizer.scales', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.28.self_attn.k_proj.weight_quantizer.zeros', 'model.layers.6.mlp.up_proj.weight_quantizer.zeros', 'model.layers.14.mlp.up_proj.bias', 'model.layers.26.self_attn.q_proj.weight_quantizer.zeros', 'model.layers.31.mlp.gate_proj.weight_quantizer.scales', 'model.layers.3.self_attn.o_proj.weight_quantizer.scales', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.30.self_attn.o_proj.weight_quantizer.zeros', 'model.layers.4.mlp.up_proj.weight_quantizer.zeros', 'model.layers.3.self_attn.k_proj.weight_quantizer.zeros', 'model.layers.26.mlp.down_proj.weight_quantizer.zeros', 'model.layers.9.mlp.up_proj.weight_quantizer.scales', 'model.layers.17.self_attn.q_proj.weight_quantizer.zeros', 'model.layers.1.mlp.up_proj.weight_quantizer.scales', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.10.self_attn.k_proj.weight_quantizer.scales', 'model.layers.31.qkt_smooth_scale', 'model.layers.4.mlp.up_proj.weight_quantizer.scales', 'model.layers.22.post_attention_layernorm.bias', 'model.layers.5.mlp.down_proj.weight_quantizer.zeros', 'model.layers.24.mlp.up_proj.bias', 'model.layers.6.self_attn.k_proj.weight_quantizer.zeros', 'model.layers.13.qkt_smooth_scale', 'model.layers.12.self_attn.k_proj.weight_quantizer.zeros', 'model.layers.25.mlp.gate_proj.weight_quantizer.zeros', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.0.self_attn.v_proj.weight_quantizer.scales', 'model.layers.25.mlp.up_proj.weight_quantizer.zeros', 'model.layers.1.mlp.up_proj.weight_quantizer.zeros', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.20.self_attn.q_proj.weight_quantizer.zeros', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.13.mlp.gate_proj.weight_quantizer.scales', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.30.self_attn.v_proj.weight_quantizer.scales', 'model.layers.19.post_attention_layernorm.bias', 'model.layers.9.mlp.down_proj.weight_quantizer.scales', 'model.layers.19.self_attn.o_proj.weight_quantizer.scales', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.31.mlp.up_proj.weight_quantizer.zeros', 'model.layers.5.self_attn.k_proj.weight_quantizer.zeros', 'model.layers.30.mlp.gate_proj.weight_quantizer.scales', 'model.layers.29.self_attn.v_proj.weight_quantizer.scales', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.8.qkt_smooth_scale', 'model.layers.13.self_attn.q_proj.weight_quantizer.zeros', 'model.layers.28.mlp.down_proj.weight_quantizer.zeros', 'model.layers.31.self_attn.o_proj.weight_quantizer.zeros', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.6.self_attn.o_proj.weight_quantizer.scales', 'model.layers.14.post_attention_layernorm.bias', 'model.layers.17.mlp.down_proj.weight_quantizer.scales', 'model.layers.26.input_layernorm.bias', 'model.layers.31.self_attn.v_proj.weight_quantizer.scales', 'model.layers.11.self_attn.k_proj.weight_quantizer.zeros', 'model.layers.19.mlp.up_proj.weight_quantizer.scales', 'model.layers.28.post_attention_layernorm.bias', 'model.layers.20.mlp.up_proj.weight_quantizer.scales', 'model.layers.30.mlp.down_proj.weight_quantizer.zeros', 'model.layers.19.mlp.down_proj.weight_quantizer.zeros', 'model.layers.16.mlp.down_proj.weight_quantizer.zeros', 'model.layers.28.mlp.up_proj.bias', 'model.layers.11.self_attn.o_proj.weight_quantizer.zeros', 'model.layers.28.self_attn.v_proj.weight_quantizer.scales', 'model.layers.29.post_attention_layernorm.bias', 'model.layers.27.mlp.gate_proj.weight_quantizer.scales', 'model.layers.27.self_attn.v_proj.weight_quantizer.scales', 'model.layers.20.mlp.up_proj.bias', 'model.layers.2.self_attn.q_proj.weight_quantizer.zeros', 'model.layers.26.self_attn.o_proj.weight_quantizer.zeros', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.21.self_attn.o_proj.weight_quantizer.zeros', 'model.layers.17.qkt_smooth_scale', 'model.layers.8.self_attn.v_proj.weight_quantizer.scales', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.25.input_layernorm.bias', 'model.layers.27.mlp.up_proj.weight_quantizer.zeros', 'model.layers.1.self_attn.k_proj.weight_quantizer.scales', 'model.layers.3.post_attention_layernorm.bias', 'model.layers.7.mlp.gate_proj.weight_quantizer.zeros', 'model.layers.22.self_attn.o_proj.weight_quantizer.zeros', 'model.layers.5.mlp.down_proj.weight_quantizer.scales', 'model.layers.18.self_attn.k_proj.weight_quantizer.scales', 'model.layers.5.self_attn.o_proj.weight_quantizer.zeros', 'model.layers.29.mlp.down_proj.weight_quantizer.scales', 'model.layers.4.input_layernorm.bias', 'model.layers.3.qkt_smooth_scale', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.4.self_attn.q_proj.weight_quantizer.scales', 'model.layers.1.mlp.down_proj.weight_quantizer.scales', 'model.layers.26.mlp.up_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.26.self_attn.v_proj.weight_quantizer.zeros', 'model.layers.6.self_attn.v_proj.weight_quantizer.zeros', 'model.layers.5.qkt_smooth_scale', 'model.layers.4.self_attn.k_proj.weight_quantizer.scales', 'model.layers.30.mlp.up_proj.weight_quantizer.zeros', 'model.layers.2.qkt_smooth_scale', 'model.layers.18.qkt_smooth_scale', 'model.layers.3.mlp.gate_proj.weight_quantizer.scales', 'model.layers.31.mlp.down_proj.weight_quantizer.scales', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.12.mlp.down_proj.weight_quantizer.zeros', 'model.layers.30.input_layernorm.bias', 'model.layers.19.input_layernorm.bias', 'model.layers.23.self_attn.o_proj.weight_quantizer.zeros', 'model.layers.25.mlp.up_proj.bias', 'model.layers.25.mlp.up_proj.weight_quantizer.scales', 'model.layers.1.self_attn.q_proj.weight_quantizer.scales', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.17.self_attn.k_proj.weight_quantizer.scales', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.22.self_attn.k_proj.weight_quantizer.scales', 'model.layers.23.mlp.gate_proj.weight_quantizer.scales', 'model.layers.11.self_attn.v_proj.weight_quantizer.zeros', 'model.layers.30.self_attn.q_proj.weight_quantizer.scales', 'model.layers.27.self_attn.o_proj.weight_quantizer.zeros', 'model.layers.22.mlp.up_proj.weight_quantizer.scales', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.31.input_layernorm.bias', 'model.layers.27.self_attn.q_proj.weight_quantizer.zeros', 'model.layers.20.self_attn.q_proj.weight_quantizer.scales', 'model.layers.25.qkt_smooth_scale', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.30.post_attention_layernorm.bias', 'model.layers.7.post_attention_layernorm.bias', 'model.layers.31.mlp.up_proj.weight_quantizer.scales', 'model.layers.0.mlp.up_proj.weight_quantizer.scales', 'model.layers.10.mlp.up_proj.weight_quantizer.zeros', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.6.post_attention_layernorm.bias', 'model.layers.13.self_attn.o_proj.weight_quantizer.scales', 'model.layers.29.self_attn.k_proj.weight_quantizer.zeros', 'model.layers.3.self_attn.o_proj.weight_quantizer.zeros', 'model.layers.24.self_attn.k_proj.weight_quantizer.scales', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.21.mlp.up_proj.weight_quantizer.zeros', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.14.self_attn.v_proj.weight_quantizer.scales', 'model.layers.21.self_attn.q_proj.weight_quantizer.zeros', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.7.self_attn.v_proj.weight_quantizer.zeros', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.14.self_attn.k_proj.weight_quantizer.zeros', 'model.layers.23.self_attn.k_proj.weight_quantizer.scales', 'model.layers.20.mlp.gate_proj.weight_quantizer.scales', 'model.layers.28.self_attn.q_proj.weight_quantizer.scales', 'model.layers.7.self_attn.q_proj.weight_quantizer.zeros', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.16.self_attn.v_proj.weight_quantizer.zeros', 'model.layers.31.mlp.down_proj.weight_quantizer.zeros', 'model.layers.23.mlp.up_proj.weight_quantizer.scales', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.21.self_attn.v_proj.weight_quantizer.zeros', 'model.layers.18.self_attn.o_proj.weight_quantizer.scales', 'model.layers.29.mlp.gate_proj.weight_quantizer.zeros', 'model.layers.13.post_attention_layernorm.bias', 'model.layers.0.self_attn.k_proj.weight_quantizer.scales', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.3.self_attn.q_proj.weight_quantizer.scales', 'model.layers.9.mlp.down_proj.weight_quantizer.zeros', 'model.layers.5.mlp.gate_proj.weight_quantizer.zeros', 'model.layers.15.self_attn.q_proj.weight_quantizer.scales', 'model.layers.14.mlp.down_proj.weight_quantizer.zeros', 'model.layers.3.self_attn.v_proj.weight_quantizer.zeros', 'model.layers.31.self_attn.o_proj.weight_quantizer.scales', 'model.layers.7.qkt_smooth_scale', 'model.layers.10.mlp.up_proj.weight_quantizer.scales', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.29.self_attn.o_proj.weight_quantizer.zeros', 'model.layers.11.self_attn.q_proj.weight_quantizer.zeros', 'model.layers.29.self_attn.o_proj.weight_quantizer.scales', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.14.mlp.gate_proj.weight_quantizer.scales', 'model.layers.19.self_attn.k_proj.weight_quantizer.scales', 'model.layers.15.mlp.down_proj.weight_quantizer.zeros', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.29.self_attn.q_proj.weight_quantizer.scales', 'model.layers.11.self_attn.q_proj.weight_quantizer.scales', 'model.layers.5.mlp.up_proj.weight_quantizer.scales', 'model.layers.18.mlp.gate_proj.weight_quantizer.scales', 'model.layers.8.self_attn.k_proj.weight_quantizer.scales', 'model.layers.14.self_attn.v_proj.weight_quantizer.zeros', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.1.self_attn.q_proj.weight_quantizer.zeros', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.27.mlp.down_proj.weight_quantizer.zeros', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.30.self_attn.o_proj.weight_quantizer.scales', 'model.layers.5.mlp.up_proj.weight_quantizer.zeros', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.5.self_attn.v_proj.weight_quantizer.zeros', 'model.layers.2.self_attn.k_proj.weight_quantizer.zeros', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.2.self_attn.k_proj.weight_quantizer.scales', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.2.self_attn.v_proj.weight_quantizer.zeros', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.8.mlp.up_proj.weight_quantizer.zeros', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.25.self_attn.v_proj.weight_quantizer.scales', 'model.layers.31.mlp.up_proj.bias', 'model.layers.18.mlp.down_proj.weight_quantizer.zeros', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.1.self_attn.o_proj.weight_quantizer.scales', 'model.layers.6.mlp.gate_proj.weight_quantizer.scales', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.6.self_attn.q_proj.weight_quantizer.zeros', 'model.layers.23.mlp.up_proj.weight_quantizer.zeros', 'model.layers.23.qkt_smooth_scale', 'model.layers.3.self_attn.q_proj.weight_quantizer.zeros', 'model.layers.27.mlp.gate_proj.weight_quantizer.zeros', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.28.self_attn.v_proj.weight_quantizer.zeros', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.30.mlp.gate_proj.weight_quantizer.zeros', 'model.layers.0.self_attn.k_proj.weight_quantizer.zeros', 'model.layers.16.self_attn.q_proj.weight_quantizer.scales', 'model.layers.21.mlp.down_proj.weight_quantizer.zeros', 'model.layers.30.self_attn.q_proj.weight_quantizer.zeros', 'model.layers.3.mlp.down_proj.weight_quantizer.scales', 'model.layers.30.self_attn.v_proj.weight_quantizer.zeros', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.9.mlp.up_proj.weight_quantizer.zeros', 'model.layers.20.self_attn.o_proj.weight_quantizer.scales', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.0.self_attn.o_proj.weight_quantizer.zeros', 'model.layers.26.mlp.up_proj.weight_quantizer.zeros', 'model.layers.2.mlp.gate_proj.weight_quantizer.zeros', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.0.self_attn.q_proj.weight_quantizer.scales', 'model.layers.23.self_attn.o_proj.weight_quantizer.scales', 'model.layers.4.self_attn.k_proj.weight_quantizer.zeros', 'model.layers.7.input_layernorm.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.8.mlp.gate_proj.weight_quantizer.scales', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.8.mlp.up_proj.weight_quantizer.scales', 'model.layers.21.self_attn.q_proj.weight_quantizer.scales', 'model.layers.6.self_attn.o_proj.weight_quantizer.zeros', 'model.layers.1.mlp.down_proj.weight_quantizer.zeros', 'model.layers.20.post_attention_layernorm.bias', 'model.layers.2.self_attn.v_proj.weight_quantizer.scales', 'model.layers.23.self_attn.q_proj.weight_quantizer.zeros', 'model.layers.24.mlp.gate_proj.weight_quantizer.scales', 'model.layers.25.mlp.down_proj.weight_quantizer.scales', 'model.layers.13.mlp.gate_proj.weight_quantizer.zeros', 'model.layers.26.mlp.gate_proj.weight_quantizer.scales', 'model.layers.7.self_attn.o_proj.weight_quantizer.zeros', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.4.mlp.down_proj.weight_quantizer.zeros', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.6.self_attn.v_proj.weight_quantizer.scales', 'model.layers.14.self_attn.o_proj.weight_quantizer.scales', 'model.layers.10.self_attn.q_proj.weight_quantizer.zeros', 'model.layers.12.mlp.up_proj.weight_quantizer.scales', 'model.layers.3.mlp.up_proj.bias', 'model.layers.31.self_attn.q_proj.weight_quantizer.scales', 'model.layers.5.mlp.gate_proj.weight_quantizer.scales', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.24.mlp.down_proj.weight_quantizer.scales', 'model.layers.11.mlp.down_proj.weight_quantizer.scales', 'model.layers.20.self_attn.k_proj.weight_quantizer.scales', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.9.self_attn.q_proj.weight_quantizer.scales', 'model.layers.24.post_attention_layernorm.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.22.self_attn.v_proj.weight_quantizer.zeros', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.17.mlp.gate_proj.weight_quantizer.scales', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.11.mlp.down_proj.weight_quantizer.zeros', 'model.layers.22.self_attn.v_proj.weight_quantizer.scales', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.12.self_attn.v_proj.weight_quantizer.scales', 'model.layers.1.self_attn.o_proj.weight_quantizer.zeros', 'model.layers.14.self_attn.q_proj.weight_quantizer.scales', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.3.mlp.gate_proj.weight_quantizer.zeros', 'model.layers.25.self_attn.k_proj.weight_quantizer.zeros', 'model.layers.26.self_attn.q_proj.weight_quantizer.scales', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.18.self_attn.o_proj.weight_quantizer.zeros', 'model.layers.9.mlp.gate_proj.weight_quantizer.zeros', 'model.layers.7.mlp.down_proj.weight_quantizer.scales', 'model.layers.20.input_layernorm.bias', 'model.layers.22.mlp.up_proj.weight_quantizer.zeros', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.25.mlp.gate_proj.weight_quantizer.scales', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.10.self_attn.v_proj.weight_quantizer.zeros', 'model.layers.2.input_layernorm.bias', 'model.layers.20.self_attn.o_proj.weight_quantizer.zeros', 'model.layers.10.post_attention_layernorm.bias', 'model.layers.20.self_attn.v_proj.weight_quantizer.scales', 'model.layers.18.mlp.gate_proj.weight_quantizer.zeros', 'model.layers.8.self_attn.v_proj.weight_quantizer.zeros', 'model.layers.12.self_attn.v_proj.weight_quantizer.zeros', 'model.layers.5.mlp.up_proj.bias', 'model.layers.29.input_layernorm.bias', 'model.layers.14.input_layernorm.bias', 'model.layers.13.self_attn.v_proj.weight_quantizer.scales', 'model.layers.11.self_attn.k_proj.weight_quantizer.scales', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.18.mlp.up_proj.weight_quantizer.zeros', 'model.layers.2.self_attn.o_proj.weight_quantizer.zeros', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.0.mlp.gate_proj.weight_quantizer.zeros', 'model.layers.5.self_attn.k_proj.weight_quantizer.scales', 'model.layers.21.self_attn.o_proj.weight_quantizer.scales', 'model.layers.19.self_attn.q_proj.weight_quantizer.zeros', 'model.layers.23.self_attn.k_proj.weight_quantizer.zeros', 'model.layers.15.self_attn.o_proj.weight_quantizer.scales', 'model.layers.24.mlp.up_proj.weight_quantizer.zeros', 'model.layers.26.mlp.down_proj.weight_quantizer.scales', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.25.post_attention_layernorm.bias', 'model.layers.20.qkt_smooth_scale', 'model.layers.5.self_attn.o_proj.weight_quantizer.scales', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.14.self_attn.o_proj.weight_quantizer.zeros', 'model.layers.20.mlp.gate_proj.weight_quantizer.zeros', 'model.layers.1.qkt_smooth_scale', 'model.layers.12.self_attn.k_proj.weight_quantizer.scales', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.12.input_layernorm.bias', 'model.layers.26.qkt_smooth_scale', 'model.layers.26.self_attn.o_proj.weight_quantizer.scales', 'model.layers.22.qkt_smooth_scale', 'model.layers.4.self_attn.o_proj.weight_quantizer.scales', 'model.layers.24.mlp.gate_proj.weight_quantizer.zeros', 'model.layers.25.self_attn.q_proj.weight_quantizer.zeros', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.13.self_attn.k_proj.weight_quantizer.scales', 'model.layers.0.post_attention_layernorm.bias', 'model.layers.12.self_attn.q_proj.weight_quantizer.zeros', 'model.layers.27.qkt_smooth_scale', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.26.self_attn.k_proj.weight_quantizer.zeros', 'model.layers.17.mlp.gate_proj.weight_quantizer.zeros', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.21.mlp.gate_proj.weight_quantizer.scales', 'model.layers.23.mlp.gate_proj.weight_quantizer.zeros', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.17.self_attn.v_proj.weight_quantizer.zeros', 'model.layers.17.mlp.up_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.2.self_attn.o_proj.weight_quantizer.scales', 'model.layers.7.self_attn.o_proj.weight_quantizer.scales', 'model.layers.24.input_layernorm.bias', 'model.layers.25.mlp.down_proj.weight_quantizer.zeros', 'model.layers.24.self_attn.o_proj.weight_quantizer.zeros', 'model.layers.29.self_attn.v_proj.weight_quantizer.zeros', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.28.qkt_smooth_scale', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.30.qkt_smooth_scale', 'model.layers.27.mlp.up_proj.weight_quantizer.scales', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.6.input_layernorm.bias', 'model.layers.19.mlp.gate_proj.weight_quantizer.zeros', 'model.layers.16.mlp.gate_proj.weight_quantizer.scales', 'model.layers.22.mlp.up_proj.bias', 'model.layers.13.self_attn.k_proj.weight_quantizer.zeros', 'model.layers.22.mlp.gate_proj.weight_quantizer.scales', 'model.layers.22.self_attn.o_proj.weight_quantizer.scales', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.20.mlp.down_proj.weight_quantizer.scales', 'model.layers.15.mlp.gate_proj.weight_quantizer.scales', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.3.mlp.up_proj.weight_quantizer.scales', 'model.layers.20.self_attn.v_proj.weight_quantizer.zeros', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.19.self_attn.k_proj.weight_quantizer.zeros', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.16.self_attn.v_proj.weight_quantizer.scales', 'model.layers.1.mlp.gate_proj.weight_quantizer.scales', 'model.layers.9.mlp.gate_proj.weight_quantizer.scales', 'model.layers.5.post_attention_layernorm.bias', 'model.layers.9.qkt_smooth_scale', 'model.layers.14.self_attn.k_proj.weight_quantizer.scales', 'model.layers.10.mlp.down_proj.weight_quantizer.scales', 'model.layers.17.mlp.down_proj.weight_quantizer.zeros', 'model.layers.10.qkt_smooth_scale', 'model.layers.12.mlp.up_proj.weight_quantizer.zeros', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.30.self_attn.k_proj.weight_quantizer.zeros', 'model.layers.16.mlp.up_proj.weight_quantizer.zeros', 'model.layers.31.self_attn.q_proj.weight_quantizer.zeros', 'model.layers.27.self_attn.v_proj.weight_quantizer.zeros', 'model.layers.4.mlp.down_proj.weight_quantizer.scales', 'model.layers.26.self_attn.v_proj.weight_quantizer.scales', 'model.layers.6.mlp.gate_proj.weight_quantizer.zeros', 'model.layers.30.mlp.up_proj.bias', 'model.layers.4.mlp.gate_proj.weight_quantizer.zeros', 'model.layers.15.mlp.down_proj.weight_quantizer.scales', 'model.layers.25.self_attn.o_proj.weight_quantizer.zeros', 'model.layers.0.self_attn.o_proj.weight_quantizer.scales', 'model.layers.8.mlp.down_proj.weight_quantizer.scales', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.1.self_attn.v_proj.weight_quantizer.scales', 'model.layers.24.mlp.up_proj.weight_quantizer.scales', 'model.layers.15.self_attn.q_proj.weight_quantizer.zeros', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.0.self_attn.v_proj.weight_quantizer.zeros', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.20.mlp.up_proj.weight_quantizer.zeros', 'model.layers.21.mlp.up_proj.weight_quantizer.scales', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.25.self_attn.k_proj.weight_quantizer.scales', 'model.layers.9.self_attn.k_proj.weight_quantizer.scales', 'model.layers.12.post_attention_layernorm.bias', 'model.layers.12.mlp.gate_proj.weight_quantizer.scales', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.29.mlp.up_proj.weight_quantizer.zeros', 'model.layers.24.self_attn.o_proj.weight_quantizer.scales', 'model.layers.14.self_attn.q_proj.weight_quantizer.zeros', 'model.layers.11.mlp.up_proj.weight_quantizer.zeros', 'model.layers.13.mlp.up_proj.weight_quantizer.zeros', 'model.layers.28.self_attn.q_proj.weight_quantizer.zeros', 'model.layers.24.self_attn.k_proj.weight_quantizer.zeros', 'model.layers.10.self_attn.q_proj.weight_quantizer.scales', 'model.layers.28.mlp.up_proj.weight_quantizer.zeros', 'model.layers.3.mlp.down_proj.weight_quantizer.zeros', 'model.layers.8.self_attn.o_proj.weight_quantizer.zeros', 'model.layers.13.mlp.up_proj.bias', 'model.layers.12.mlp.gate_proj.weight_quantizer.zeros', 'model.layers.10.mlp.gate_proj.weight_quantizer.scales', 'model.layers.28.mlp.gate_proj.weight_quantizer.scales', 'model.layers.29.self_attn.q_proj.weight_quantizer.zeros', 'model.layers.8.input_layernorm.bias', 'model.layers.27.self_attn.q_proj.weight_quantizer.scales', 'model.layers.9.self_attn.k_proj.weight_quantizer.zeros', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.1.input_layernorm.bias', 'model.layers.13.self_attn.v_proj.weight_quantizer.zeros', 'model.layers.11.self_attn.o_proj.weight_quantizer.scales', 'model.layers.20.mlp.down_proj.weight_quantizer.zeros', 'model.layers.0.mlp.up_proj.weight_quantizer.zeros', 'model.layers.0.mlp.gate_proj.weight_quantizer.scales', 'model.layers.24.self_attn.q_proj.weight_quantizer.zeros', 'model.layers.27.self_attn.k_proj.weight_quantizer.scales', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.14.qkt_smooth_scale', 'model.layers.23.self_attn.v_proj.weight_quantizer.scales', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.19.self_attn.v_proj.weight_quantizer.scales', 'model.layers.13.self_attn.o_proj.weight_quantizer.zeros', 'model.layers.18.self_attn.k_proj.weight_quantizer.zeros', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.7.mlp.down_proj.weight_quantizer.zeros', 'model.layers.15.mlp.up_proj.weight_quantizer.zeros', 'model.layers.15.qkt_smooth_scale', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.19.self_attn.v_proj.weight_quantizer.zeros', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.13.mlp.down_proj.weight_quantizer.scales', 'model.layers.22.mlp.gate_proj.weight_quantizer.zeros', 'model.layers.11.mlp.gate_proj.weight_quantizer.scales', 'model.layers.28.mlp.up_proj.weight_quantizer.scales', 'model.layers.29.mlp.down_proj.weight_quantizer.zeros', 'model.layers.17.mlp.up_proj.weight_quantizer.zeros', 'model.layers.18.self_attn.v_proj.weight_quantizer.zeros', 'model.layers.6.self_attn.q_proj.weight_quantizer.scales', 'model.layers.29.self_attn.k_proj.weight_quantizer.scales', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.8.self_attn.o_proj.weight_quantizer.scales', 'model.layers.26.mlp.up_proj.weight_quantizer.scales', 'model.layers.19.mlp.gate_proj.weight_quantizer.scales', 'model.layers.3.self_attn.v_proj.weight_quantizer.scales', 'model.layers.7.self_attn.v_proj.weight_quantizer.scales', 'model.layers.15.self_attn.v_proj.weight_quantizer.scales', 'model.layers.13.input_layernorm.bias', 'model.layers.27.self_attn.k_proj.weight_quantizer.zeros', 'model.layers.2.self_attn.q_proj.weight_quantizer.scales', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.3.self_attn.k_proj.weight_quantizer.scales', 'model.layers.14.mlp.down_proj.weight_quantizer.scales', 'model.layers.5.input_layernorm.bias', 'model.layers.14.mlp.gate_proj.weight_quantizer.zeros', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.10.self_attn.k_proj.weight_quantizer.zeros', 'model.layers.23.mlp.down_proj.weight_quantizer.zeros', 'model.layers.16.mlp.gate_proj.weight_quantizer.zeros', 'model.layers.16.mlp.down_proj.weight_quantizer.scales', 'model.layers.4.self_attn.v_proj.weight_quantizer.scales', 'model.layers.5.self_attn.q_proj.weight_quantizer.scales', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.10.mlp.gate_proj.weight_quantizer.zeros', 'model.layers.17.self_attn.o_proj.weight_quantizer.zeros', 'model.layers.23.self_attn.v_proj.weight_quantizer.zeros', 'model.layers.23.input_layernorm.bias', 'model.layers.8.self_attn.k_proj.weight_quantizer.zeros', 'model.layers.0.input_layernorm.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.21.self_attn.k_proj.weight_quantizer.scales', 'model.layers.6.mlp.down_proj.weight_quantizer.zeros', 'model.layers.2.mlp.down_proj.weight_quantizer.scales', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.18.self_attn.q_proj.weight_quantizer.scales', 'model.layers.16.self_attn.k_proj.weight_quantizer.zeros', 'model.layers.17.input_layernorm.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.5.self_attn.q_proj.weight_quantizer.zeros', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.31.self_attn.k_proj.weight_quantizer.scales', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.2.mlp.down_proj.weight_quantizer.zeros', 'model.layers.10.input_layernorm.bias', 'model.layers.17.mlp.up_proj.weight_quantizer.scales', 'model.layers.8.self_attn.q_proj.weight_quantizer.zeros', 'model.layers.1.post_attention_layernorm.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.17.self_attn.o_proj.weight_quantizer.scales', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.29.mlp.up_proj.weight_quantizer.scales', 'model.layers.16.self_attn.q_proj.weight_quantizer.zeros', 'model.layers.15.mlp.gate_proj.weight_quantizer.zeros', 'model.layers.12.self_attn.q_proj.weight_quantizer.scales', 'model.layers.17.post_attention_layernorm.bias', 'model.layers.11.mlp.gate_proj.weight_quantizer.zeros', 'model.layers.15.mlp.up_proj.weight_quantizer.scales', 'model.layers.16.input_layernorm.bias', 'model.layers.20.self_attn.k_proj.weight_quantizer.zeros', 'model.layers.27.self_attn.o_proj.weight_quantizer.scales', 'model.layers.29.qkt_smooth_scale', 'model.layers.3.input_layernorm.bias', 'model.layers.11.input_layernorm.bias', 'model.layers.0.self_attn.q_proj.weight_quantizer.zeros', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.24.mlp.down_proj.weight_quantizer.zeros', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.10.mlp.down_proj.weight_quantizer.zeros', 'model.layers.24.self_attn.q_proj.weight_quantizer.scales', 'model.layers.28.input_layernorm.bias', 'model.layers.15.post_attention_layernorm.bias', 'model.layers.19.mlp.up_proj.weight_quantizer.zeros', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.18.self_attn.v_proj.weight_quantizer.scales', 'model.layers.2.mlp.up_proj.weight_quantizer.scales', 'model.layers.16.self_attn.o_proj.weight_quantizer.scales', 'model.layers.26.post_attention_layernorm.bias', 'model.layers.19.self_attn.o_proj.weight_quantizer.zeros', 'model.layers.23.mlp.up_proj.bias', 'model.layers.31.post_attention_layernorm.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.4.mlp.gate_proj.weight_quantizer.scales', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.21.input_layernorm.bias', 'model.layers.16.self_attn.k_proj.weight_quantizer.scales', 'model.layers.29.mlp.gate_proj.weight_quantizer.scales', 'model.layers.21.mlp.down_proj.weight_quantizer.scales', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.19.qkt_smooth_scale', 'model.layers.4.qkt_smooth_scale', 'model.layers.13.mlp.down_proj.weight_quantizer.zeros', 'model.layers.0.mlp.down_proj.weight_quantizer.scales', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.9.self_attn.o_proj.weight_quantizer.scales']\n",
      "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False,legacy=False)\n",
    "model: LlamaForCausalLM = LlamaForCausalLM.from_pretrained(model_path, config=config, device_map='cpu',torch_dtype=torch.float16,)\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff2e73b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader = torch.load('../cache/llama/wikitext2_128/dataloader.cache')\n",
    "inps0: torch.Tensor = torch.load(f'../cache/llama/wikitext2_128/inps_0.cache',map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a57c8159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2147483648\n"
     ]
    }
   ],
   "source": [
    "def get_tensor_memory_size(tensor: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Calculate the memory size of a tensor in bytes.\n",
    "    \"\"\"\n",
    "    return tensor.element_size() * tensor.nelement()\n",
    "print(get_tensor_memory_size(inps0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d0bad2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0292, -0.0015,  0.0145,  ..., -0.0018,  0.0266, -0.0109],\n",
      "        [-0.0012,  0.0057, -0.0074,  ..., -0.0071, -0.0054,  0.0093],\n",
      "        [-0.0043,  0.0106, -0.0044,  ...,  0.0034, -0.0136,  0.0223],\n",
      "        ...,\n",
      "        [ 0.0154,  0.0178,  0.0023,  ..., -0.0134,  0.0190,  0.0032],\n",
      "        [-0.0061,  0.0127, -0.0095,  ..., -0.0065,  0.0168, -0.0006],\n",
      "        [-0.0198,  0.0035,  0.0143,  ..., -0.0118, -0.0069, -0.0058]],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.HalfTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m inp \u001b[38;5;241m=\u001b[39m inps0[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(dev)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(inp)\n\u001b[0;32m----> 5\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(out)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Free up memory\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/lrquant/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/lrquant/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/lrquant/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:806\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    803\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    805\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 806\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/workspace/lrquant/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/lrquant/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/lrquant/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:646\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    643\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m position_ids\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, seq_length)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 646\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;66;03m# embed positions\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/workspace/lrquant/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/lrquant/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/lrquant/.venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/lrquant/.venv/lib/python3.10/site-packages/torch/nn/functional.py:2237\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2233\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2236\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.HalfTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "model.to(dev)\n",
    "\n",
    "inp = inps0[0].to(dev)\n",
    "print(inp)\n",
    "out = model(inp)\n",
    "print(out)\n",
    "\n",
    "# Free up memory\n",
    "del inp\n",
    "del out\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee985e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1589789184 42074112 54525952\n",
      "0 1589789184 42074112 71303168\n",
      "1 1589789184 42074112 75497472\n",
      "2 1589789184 42074112 75497472\n",
      "3 1589789184 42074112 75497472\n",
      "4 1589789184 42074112 75497472\n",
      "5 1589789184 42074112 75497472\n",
      "6 1589789184 42074112 75497472\n",
      "7 1589789184 42074112 75497472\n",
      "8 1589789184 42074112 75497472\n",
      "9 1589789184 42074112 75497472\n",
      "10 1589789184 42074112 75497472\n",
      "11 1589789184 42074112 75497472\n",
      "12 1589789184 42074112 75497472\n",
      "13 1589789184 42074112 75497472\n",
      "14 1589789184 42074112 75497472\n",
      "15 1589789184 42074112 75497472\n",
      "16 1589789184 42074112 75497472\n",
      "17 1589789184 42074112 75497472\n",
      "18 1589789184 42074112 75497472\n",
      "19 1589789184 42074112 75497472\n",
      "20 1589789184 42074112 75497472\n",
      "21 1589789184 42074112 75497472\n",
      "22 1589789184 42074112 75497472\n",
      "23 1589789184 42074112 75497472\n",
      "24 1589789184 42074112 75497472\n",
      "25 1589789184 42074112 75497472\n",
      "26 1589789184 42074112 75497472\n",
      "27 1589789184 42074112 75497472\n",
      "28 1589789184 42074112 75497472\n",
      "29 1589789184 42074112 75497472\n",
      "30 1589789184 42074112 75497472\n",
      "31 1589789184 42074112 75497472\n"
     ]
    }
   ],
   "source": [
    "# print(\n",
    "#     torch.cuda.max_memory_allocated(),\n",
    "#     torch.cuda.memory_allocated(),\n",
    "#     torch.cuda.memory_reserved(),\n",
    "# )\n",
    "inp = inps0[0].unsqueeze(0).to(dev)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(32):\n",
    "        layer = model.model.layers[i].to(dev)\n",
    "        inp: torch.Tensor = layer(inp)[0]\n",
    "        layer.cpu()\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c468fbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([29889, 29889,    13,  ..., 29889, 29889, 29889], device='cuda:0')\n",
      "..\n",
      ".......\n",
      "...\n",
      ".....\n",
      ".\n",
      "...\n",
      "...\n",
      "\n",
      "..........\n",
      ".............\n",
      "....\n",
      "................\n",
      "...\n",
      "...\n",
      ".\n",
      "\n",
      "......\n",
      ".\n",
      ".......\n",
      "...\n",
      ".\n",
      ".................................\n",
      "............\n",
      "...............................\n",
      "................................\n",
      "............\n",
      ".\n",
      "...\n",
      ".\n",
      "..\n",
      "......\n",
      "....\n",
      "..\n",
      "...\n",
      ".....\n",
      "....\n",
      "\n",
      ".\n",
      "......\n",
      "....\n",
      ".......\n",
      ".....\n",
      "............\n",
      "....\n",
      "..\n",
      ".............\n",
      "....\n",
      "....\n",
      ".......\n",
      "...........\n",
      "........\n",
      ".\n",
      "...\n",
      ".....\n",
      ".............\n",
      "..\n",
      "....\n",
      ".....\n",
      "..........\n",
      "\n",
      ".....\n",
      "........\n",
      ".........\n",
      "...........\n",
      "...\n",
      "....\n",
      "....\n",
      "\n",
      "................................\n",
      "..\n",
      "...............\n",
      ".......................\n",
      "...........\n",
      "....\n",
      ".\n",
      "\n",
      "...\n",
      ".............\n",
      "\n",
      "..................\n",
      "........\n",
      "........\n",
      ".......\n",
      "\n",
      "\n",
      "...............\n",
      "\n",
      ".\n",
      ".\n",
      "..\n",
      "..\n",
      ".................\n",
      "\n",
      ".........\n",
      "........\n",
      "\n",
      ".................................\n",
      "..........\n",
      ".\n",
      ".\n",
      ".\n",
      ".....\n",
      "..............\n",
      "...\n",
      ".......\n",
      ".............\n",
      "..................................\n",
      "............\n",
      "...........................\n",
      ".............\n",
      "......\n",
      "........\n",
      ".....\n",
      ".....................\n",
      "...........\n",
      "...........\n",
      "\n",
      "...........\n",
      "..........\n",
      "................\n",
      ".........\n",
      ".......\n",
      "....\n",
      "....\n",
      "........\n",
      "...........................................\n",
      ".......\n",
      "................\n",
      "\n",
      ".............................\n",
      "..\n",
      "....\n",
      "........\n",
      "................\n",
      "....................\n",
      "....\n",
      "...\n",
      ".....................\n",
      "....\n",
      ".\n",
      "...............\n",
      "\n",
      "..\n",
      "\n",
      ".........\n",
      "....\n",
      "..\n",
      "........\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "...........\n",
      ".....\n",
      "......\n",
      "...........\n",
      "...........\n",
      "...\n",
      "\n",
      "........\n",
      "..............................\n",
      "....\n",
      ".....\n",
      "........\n",
      "........\n",
      "...\n",
      ".....\n",
      "...........\n",
      "...\n",
      "...\n",
      "..\n",
      "..\n",
      "......\n",
      "\n",
      "..\n",
      "..\n",
      "\n",
      "........\n",
      "\n",
      "............\n",
      "........\n",
      ".......\n",
      "...\n",
      ".......\n",
      "...\n",
      "...\n",
      "....\n",
      ".......\n",
      "....\n",
      "......\n",
      "......\n",
      "\n",
      "...........................\n",
      "...........\n",
      "....\n",
      "..................\n",
      "....\n",
      "......\n",
      "..........\n",
      ".....\n",
      ".\n",
      "............\n",
      "\n",
      "\n",
      ".......\n",
      ".....\n",
      "...\n",
      "...\n",
      "\n",
      "..\n",
      "\n",
      "............\n",
      ".......\n",
      "\n",
      "...............\n",
      "...........\n",
      "\n",
      "....\n",
      "\n",
      "..\n",
      "....\n",
      "\n",
      ".\n",
      "......\n",
      "\n",
      "........................\n",
      "...\n",
      "..\n",
      "\n",
      ".........\n",
      "................\n",
      "...............\n",
      "......\n",
      ".....\n",
      "...........\n",
      ".................\n",
      ".........\n",
      ".....\n",
      ".........\n",
      ".......\n",
      ".........................................................\n",
      "........\n",
      "..................\n",
      ".......\n",
      ".......\n",
      "....................\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=4096, out_features=32000, bias=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head.to(dev)\n",
    "with torch.no_grad():\n",
    "    logits = model.lm_head(inp)\n",
    "    logits = logits[0].argmax(dim=1)\n",
    "    print(logits)\n",
    "    print(tokenizer.decode(logits, skip_special_tokens=True))\n",
    "model.lm_head.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a84bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
